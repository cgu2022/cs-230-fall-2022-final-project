{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "72bf15ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import audiomentations\n",
    "#from audiomentations import Compose, AddGaussianNoise, PitchShift\n",
    "#import torch_audiomentations\n",
    "from torch_audiomentations import Compose, PitchShift, TimeInversion\n",
    "import torchaudio\n",
    "import numpy as np\n",
    "import sys\n",
    "\n",
    "sys.path.insert(0, '../src')\n",
    "from segmentation import segment_cough\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "a6fe837a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adapted from https://github.com/musikalkemist/pytorchforaudio\n",
    "class CoughDataset(Dataset):\n",
    "\n",
    "    def __init__(self,\n",
    "                 annotations_df,\n",
    "                 audio_dir,\n",
    "                #  transformation,\n",
    "                 target_sample_rate,\n",
    "                 num_samples,\n",
    "                 device,\n",
    "                #  segment=False,\n",
    "                #  augment=False,\n",
    "                ):\n",
    "        self.annotations = annotations_df\n",
    "        self.audio_dir = audio_dir\n",
    "        self.device = device\n",
    "        # self.transformation = transformation.to(self.device)\n",
    "        self.target_sample_rate = target_sample_rate\n",
    "        self.num_samples = num_samples\n",
    "        self.label_dict = {'healthy':0, 'symptomatic':1, 'COVID-19':2}\n",
    "        self.label_weights = self._calculate_weights(annotations_df)\n",
    "        \n",
    "        # self.do_segment = segment\n",
    "\n",
    "        # self.segmentation = segment_cough\n",
    "\n",
    "        # self.do_augment = augment\n",
    "        # self.augmentations = Compose(\n",
    "        #         [\n",
    "        #             AddGaussianNoise(min_amplitude=0.01, max_amplitude=0.05, p=0.5),\n",
    "        #             PitchShift(min_semitones=-8, max_semitones=8, p=0.5)\n",
    "        #         ]\n",
    "        # )\n",
    "        # self.augmentations = Compose(\n",
    "        #     transforms=[\n",
    "        #         PitchShift(\n",
    "        #             mode = \"per_example\",\n",
    "        #             p=0.5,\n",
    "        #             sample_rate=self.target_sample_rate\n",
    "        #             ),\n",
    "        #         TimeInversion(\n",
    "        #             mode = \"per_example\",\n",
    "        #             p=0.5,\n",
    "        #         ),\n",
    "        #     ]\n",
    "        # )\n",
    "\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.annotations)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        audio_sample_path = self._get_audio_sample_path(index)\n",
    "        label = self.label_dict[self._get_audio_sample_label(index)]\n",
    "        signal, sr = torchaudio.load(audio_sample_path)\n",
    "\n",
    "        # if self.do_segment:\n",
    "        #     segments, segment_mask = self.segmentation(signal.numpy()[0], float(sr))\n",
    "        #     #print('segment length: ', len(segments))\n",
    "        #     if len(segments) > 0:\n",
    "        #         signal = torch.tensor(segments[0])\n",
    "        #         signal = signal.unsqueeze(0)\n",
    "        #     else:\n",
    "        #         signal = signal\n",
    "        #     #print('signal in self.do_segment: ', signal)\n",
    "\n",
    "        \n",
    "        signal = signal.to(self.device)\n",
    "        signal = self._resample_if_necessary(signal, sr)\n",
    "        signal = self._mix_down_if_necessary(signal)\n",
    "        signal = self._cut_if_necessary(signal)\n",
    "        signal = self._right_pad_if_necessary(signal)\n",
    "\n",
    "        # if self.do_augment:\n",
    "        #     #signal = torch.from_numpy(self.augmentations(signal.numpy(), sr))\n",
    "        #     # print('signal shape: ', signal.shape)\n",
    "        #     # print('sample rate: ', sr)\n",
    "        #     # print('target sample rate: ', self.target_sample_rate)\n",
    "        #     # add a 1 in front of the signal array\n",
    "        #     signal = signal.unsqueeze(0)\n",
    "        #     signal = self.augmentations(signal, self.target_sample_rate)\n",
    "        #     # remove the 1 in front of the signal array\n",
    "        #     signal = signal.squeeze(0)\n",
    "        \n",
    "\n",
    "        # signal = self.transformation(signal)\n",
    "\n",
    "        \n",
    "        return signal, label\n",
    "\n",
    "    def _cut_if_necessary(self, signal):\n",
    "        if signal.shape[1] > self.num_samples:\n",
    "            signal = signal[:, :self.num_samples]\n",
    "        return signal\n",
    "\n",
    "    def _right_pad_if_necessary(self, signal):\n",
    "        length_signal = signal.shape[1]\n",
    "        if length_signal < self.num_samples:\n",
    "            num_missing_samples = self.num_samples - length_signal\n",
    "            last_dim_padding = (0, num_missing_samples)\n",
    "            signal = torch.nn.functional.pad(signal, last_dim_padding)\n",
    "        return signal\n",
    "\n",
    "    def _resample_if_necessary(self, signal, sr):\n",
    "        #print('resampled signal')\n",
    "        if sr != self.target_sample_rate:\n",
    "            resampler = torchaudio.transforms.Resample(sr, self.target_sample_rate)\n",
    "            signal = resampler(signal)\n",
    "        return signal\n",
    "\n",
    "    def _mix_down_if_necessary(self, signal):\n",
    "        if signal.shape[0] > 1:\n",
    "            signal = torch.mean(signal, dim=0, keepdim=True)\n",
    "        return signal\n",
    "\n",
    "    def _get_audio_sample_path(self, index):\n",
    "        path = os.path.join(self.audio_dir, self.annotations.iloc[index, 0])+\".wav\"\n",
    "        return path\n",
    "\n",
    "    def _get_audio_sample_label(self, index):\n",
    "        return self.annotations.iloc[index, 9]\n",
    "\n",
    "    def _calculate_weights(self, annotation_df):\n",
    "        counts = annotation_df[\"status\"].value_counts()\n",
    "        total = len(annotation_df)\n",
    "        weights = (1-(counts/total))\n",
    "        weights /= weights.sum()\n",
    "        return torch.FloatTensor(weights)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "d5a17a8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "AUDIO_DIR = \"../valid_data/\"\n",
    "SAMPLE_RATE = 16000\n",
    "NUM_SAMPLES = SAMPLE_RATE*10\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    device = \"cuda\"\n",
    "else:\n",
    "    device = \"cpu\"\n",
    "# print(f\"Using device {device}\")\n",
    "\n",
    "# train_df = pd.read_parquet(os.path.join(AUDIO_DIR, \"train.parquet.gzip\"))\n",
    "# val_df = pd.read_parquet(os.path.join(AUDIO_DIR, \"val.parquet.gzip\"))\n",
    "# test_df = pd.read_parquet(os.path.join(AUDIO_DIR, \"test.parquet.gzip\"))\n",
    "\n",
    "# train_df = pd.read_parquet(os.path.join(AUDIO_DIR, \"train_edited.parquet.gzip\"))\n",
    "# val_df = pd.read_parquet(os.path.join(AUDIO_DIR, \"val_edited.parquet.gzip\"))\n",
    "# test_df = pd.read_parquet(os.path.join(AUDIO_DIR, \"test_edited.parquet.gzip\"))\n",
    "\n",
    "train_df = pd.read_parquet(os.path.join(AUDIO_DIR, \"train_balanced.parquet.gzip\"))\n",
    "val_df = pd.read_parquet(os.path.join(AUDIO_DIR, \"val_balanced.parquet.gzip\"))\n",
    "test_df = pd.read_parquet(os.path.join(AUDIO_DIR, \"test_balanced.parquet.gzip\"))\n",
    "\n",
    "\n",
    "# print(f\"There are {len(usd)} samples in the dataset.\")\n",
    "# signal, label = usd[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "c258e123",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNNNetwork(torch.nn.Module):\n",
    "\n",
    "    def __init__(self, drop_p=0.2):\n",
    "        super().__init__()\n",
    "        # 4 conv blocks / flatten / linear / softmax\n",
    "\n",
    "        self.conv1 = nn.Sequential(\n",
    "            \n",
    "            nn.Conv2d(\n",
    "                in_channels=1,\n",
    "                out_channels=16,\n",
    "                kernel_size=3,\n",
    "                stride=2,\n",
    "                padding=0\n",
    "            ),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2),\n",
    "            nn.BatchNorm2d(16),\n",
    "            nn.Dropout(p=drop_p)\n",
    "\n",
    "        )\n",
    "\n",
    "\n",
    "        self.conv2 = nn.Sequential(\n",
    "            \n",
    "            nn.Conv2d(\n",
    "                in_channels=16,\n",
    "                out_channels=32,\n",
    "                kernel_size=3,\n",
    "                stride=2,\n",
    "                padding=0\n",
    "            ),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=1),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.Dropout(p=drop_p)\n",
    "        )\n",
    "\n",
    "        self.conv3 = nn.Sequential(\n",
    "            \n",
    "            nn.Conv2d(\n",
    "                in_channels=32,\n",
    "                out_channels=64,\n",
    "                kernel_size=3,\n",
    "                stride=1,\n",
    "                padding=0\n",
    "            ),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=1),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.Dropout(p=drop_p)\n",
    "        )\n",
    " \n",
    "\n",
    "        self.conv4 = nn.Sequential(\n",
    "            \n",
    "            nn.Conv2d(\n",
    "                in_channels=64,\n",
    "                out_channels=128,\n",
    "                kernel_size=3,\n",
    "                stride=1,\n",
    "                padding=0\n",
    "            ),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=1),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.Dropout(p=drop_p)\n",
    "        )\n",
    "\n",
    "\n",
    "        self.flatten = nn.Flatten()\n",
    "        \n",
    "        self.linear = nn.Linear(31744, 3)\n",
    "        \n",
    "        self.softmax = nn.Softmax(dim=-1)\n",
    "\n",
    "    def forward(self, input_data):\n",
    "        #nomralization\n",
    "        std = input_data.std()\n",
    "        input_data -= input_data.mean()\n",
    "        input_data /= std\n",
    "        \n",
    "        x = self.conv1(input_data)\n",
    "        x = self.conv2(x)\n",
    "        x = self.conv3(x)\n",
    "        x = self.conv4(x)\n",
    "        x = self.flatten(x)\n",
    "        logits = self.linear(x)\n",
    "        predictions = self.softmax(logits)\n",
    "        return predictions\n",
    "    \n",
    "    def _init_weights(self):\n",
    "        if isinstance(self, nn.Linear):\n",
    "            torch.nn.init.xavier_uniform(self.weight)\n",
    "            self.bias.data.fill_(0.01)\n",
    "        elif isinstance(self, nn.Conv2d):\n",
    "            torch.nn.init.xavier_uniform(self.weight)\n",
    "            self.bias.data.fill_(0.01)\n",
    "        elif isinstance(self, nn.BatchNorm2d):\n",
    "            self.weight.data.fill_(1)\n",
    "            self.bias.data.zero_()\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "2c406986",
   "metadata": {},
   "outputs": [],
   "source": [
    "mel_spectrogram = torchaudio.transforms.MelSpectrogram(\n",
    "    sample_rate=SAMPLE_RATE,\n",
    "    n_fft=1024,\n",
    "    hop_length=512,\n",
    "    n_mels=128\n",
    ").to(device)\n",
    "\n",
    "def create_data_loader(train_data, batch_size):\n",
    "    train_dataloader = DataLoader(train_data, batch_size=batch_size)\n",
    "    print('train_dataloader finished: ', train_dataloader)\n",
    "    return train_dataloader\n",
    "\n",
    "def count_correct(logits, y_true):\n",
    "    y_pred = torch.argmax(logits, axis = 1)\n",
    "    return torch.sum(y_pred==y_true)\n",
    "\n",
    "def train_single_epoch(model, train_data_loader, val_data_loader, loss_fn, optimiser, device, do_augment=False):\n",
    "    total_loss = 0.0\n",
    "    correct_pred = 0.0\n",
    "    total_pred = 0\n",
    "\n",
    "    augmentations = Compose(\n",
    "            transforms=[\n",
    "                PitchShift(\n",
    "                    mode = \"per_example\",\n",
    "                    p=0.5,\n",
    "                    sample_rate=SAMPLE_RATE\n",
    "                    ),\n",
    "                TimeInversion(\n",
    "                    mode = \"per_example\",\n",
    "                    p=0.5,\n",
    "                ),\n",
    "            ]\n",
    "        )\n",
    "\n",
    "    for x_batch, y_batch in tqdm(train_data_loader):\n",
    "        x_batch, y_batch = x_batch.to(device), y_batch.to(device)\n",
    "        #print('x_batch before augment: ', x_batch.shape)\n",
    "\n",
    "        if do_augment:\n",
    "            x_batch = augmentations(x_batch, SAMPLE_RATE)\n",
    "        \n",
    "        #print('x_batch after augment: ', x_batch.shape)\n",
    "\n",
    "        # convert (batch, channel, time)-waveform into (batch*channel, time)-waveform, apply the mel spectrogram transform, and then convert back.\n",
    "        x_batch = x_batch.reshape(-1, x_batch.shape[-1])\n",
    "        #print('x_batch after (batch*channel, time): ', x_batch.shape)\n",
    "        x_batch = mel_spectrogram(x_batch)\n",
    "        #print('x_batch after mel: ', x_batch.shape)\n",
    "        x_batch = x_batch.reshape(x_batch.shape[0], 1, x_batch.shape[-2], x_batch.shape[-1])\n",
    "        #print('x_batch: ', x_batch.shape)\n",
    "        \n",
    "        # calculate loss\n",
    "        y_pred = model(x_batch)\n",
    "        loss = loss_fn(y_pred, y_batch)\n",
    "        \n",
    "        correct_pred += count_correct(y_pred, y_batch)\n",
    "        total_pred += y_batch.shape[0]\n",
    "\n",
    "        # backpropagate error and update weights\n",
    "        optimiser.zero_grad()\n",
    "        loss.backward()\n",
    "        optimiser.step()\n",
    "        total_loss += loss.item()\n",
    "        \n",
    "    print(f\"Training loss: {total_loss}, Training accuracy : {correct_pred/total_pred}\")\n",
    "    \n",
    "    total_loss = 0.0\n",
    "    correct_pred = 0.0\n",
    "    total_pred = 0\n",
    "    for x_batch, y_batch in tqdm(val_data_loader):\n",
    "        with torch.no_grad():\n",
    "            x_batch, y_batch = x_batch.to(device), y_batch.to(device)\n",
    "\n",
    "            x_batch = x_batch.reshape(-1, x_batch.shape[-1])\n",
    "            #print('x_batch after (batch*channel, time): ', x_batch.shape)\n",
    "            x_batch = mel_spectrogram(x_batch)\n",
    "            #print('x_batch after mel: ', x_batch.shape)\n",
    "            x_batch = x_batch.reshape(x_batch.shape[0], 1, x_batch.shape[-2], x_batch.shape[-1])\n",
    "\n",
    "            y_pred = model(x_batch)\n",
    "            loss = loss_fn(y_pred, y_batch)\n",
    "            total_loss += loss.item() \n",
    "            \n",
    "        correct_pred += count_correct(y_pred, y_batch)\n",
    "        total_pred += y_batch.shape[0]\n",
    "        \n",
    "    print(f\"Validataion loss: {total_loss}, Validation accuracy : {correct_pred/total_pred}\")\n",
    "\n",
    "    \n",
    "def train(model, train_data_loader, val_data_loader, loss_fn, optimiser, device, epochs, do_augment):\n",
    "    for i in range(epochs):\n",
    "        print(f\"Epoch {i+1}\")\n",
    "        train_single_epoch(model, train_data_loader, val_data_loader, loss_fn, optimiser, device)\n",
    "        \n",
    "        path = os.path.join(MODEL_FOLDER, f\"epoch_{i}.pth\")\n",
    "        torch.save(model.state_dict(), path)\n",
    "        print(f\"Saved at {path}\")\n",
    "        print(\"---------------------------\")\n",
    "    print(\"Finished training\")\n",
    "    print(\"---------------------------\")\n",
    "    \n",
    "    \n",
    "def evaluate(model, eval_data_loader, loss_fn, device):\n",
    "    print(\"Evaluating model\")\n",
    "    total_loss = 0.0\n",
    "    correct_pred = 0.0\n",
    "    total_pred = 0\n",
    "    for x_batch, y_batch in tqdm(eval_data_loader):\n",
    "        with torch.no_grad():\n",
    "            x_batch, y_batch = x_batch.to(device), y_batch.to(device)\n",
    "\n",
    "            x_batch = x_batch.reshape(-1, x_batch.shape[-1])\n",
    "            #print('x_batch after (batch*channel, time): ', x_batch.shape)\n",
    "            x_batch = mel_spectrogram(x_batch)\n",
    "            #print('x_batch after mel: ', x_batch.shape)\n",
    "            x_batch = x_batch.reshape(x_batch.shape[0], 1, x_batch.shape[-2], x_batch.shape[-1])\n",
    "\n",
    "            # calculate loss\n",
    "            y_pred = model(x_batch)\n",
    "            loss = loss_fn(y_pred, y_batch)\n",
    "\n",
    "            correct_pred += count_correct(y_pred, y_batch)\n",
    "            total_pred += y_batch.shape[0]\n",
    "\n",
    "            total_loss += loss.item()\n",
    "        \n",
    "    print(f\"Evaluation loss: {total_loss}, Evaluation accuracy : {correct_pred/total_pred}\")\n",
    "    print(\"---------------------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "b393f0aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train data:  <__main__.CoughDataset object at 0x2a1e30e50>\n",
      "train_dataloader finished:  <torch.utils.data.dataloader.DataLoader object at 0x2a73e8d90>\n",
      "train_dataloader finished:  <torch.utils.data.dataloader.DataLoader object at 0x2a73e9a20>\n",
      "train_dataloader finished:  <torch.utils.data.dataloader.DataLoader object at 0x2a1e30f40>\n"
     ]
    }
   ],
   "source": [
    "BATCH_SIZE =16\n",
    "EPOCHS = 10\n",
    "MODEL_FOLDER = '../models/'\n",
    "\n",
    "# mel_spectrogram = torchaudio.transforms.MelSpectrogram(\n",
    "#     sample_rate=SAMPLE_RATE,\n",
    "#     n_fft=1024,\n",
    "#     hop_length=512,\n",
    "#     n_mels=128\n",
    "# )\n",
    "\n",
    "train_data = CoughDataset(train_df,\n",
    "                        AUDIO_DIR,\n",
    "                        # mel_spectrogram,\n",
    "                        SAMPLE_RATE,\n",
    "                        NUM_SAMPLES,\n",
    "                        device,\n",
    "                        # augment = True\n",
    "                        )\n",
    "print('train data: ', train_data)\n",
    "\n",
    "val_data = CoughDataset(val_df,\n",
    "                        AUDIO_DIR,\n",
    "                        # mel_spectrogram,\n",
    "                        SAMPLE_RATE,\n",
    "                        NUM_SAMPLES,\n",
    "                        device)\n",
    "\n",
    "test_data = CoughDataset(test_df,\n",
    "                        AUDIO_DIR,\n",
    "                        # mel_spectrogram,\n",
    "                        SAMPLE_RATE,\n",
    "                        NUM_SAMPLES,\n",
    "                        device)\n",
    "\n",
    "train_dataloader = create_data_loader(train_data, BATCH_SIZE)\n",
    "val_dataloader = create_data_loader(val_data, BATCH_SIZE)\n",
    "test_dataloader = create_data_loader(val_data, BATCH_SIZE)\n",
    "\n",
    "# construct model and assign it to device\n",
    "model = CNNNetwork().to(device)\n",
    "\n",
    "# initialise loss funtion + optimiser\n",
    "#loss_fn = nn.CrossEntropyLoss(weight=train_data.label_weights)\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimiser = torch.optim.Adam(model.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "849e78e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 87/87 [00:13<00:00,  6.47it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss: 86.6779094338417, Training accuracy : 0.5468409657478333\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:01<00:00,  7.95it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validataion loss: 9.751448929309845, Validation accuracy : 0.5882353186607361\n",
      "Saved at ../models/epoch_0.pth\n",
      "---------------------------\n",
      "Epoch 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 87/87 [00:13<00:00,  6.37it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss: 84.96021908521652, Training accuracy : 0.5700798630714417\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:01<00:00,  6.32it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validataion loss: 9.74361115694046, Validation accuracy : 0.5882353186607361\n",
      "Saved at ../models/epoch_1.pth\n",
      "---------------------------\n",
      "Epoch 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 87/87 [00:13<00:00,  6.34it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss: 84.03860372304916, Training accuracy : 0.5802469253540039\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:01<00:00,  7.67it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validataion loss: 9.806114733219147, Validation accuracy : 0.5816993713378906\n",
      "Saved at ../models/epoch_2.pth\n",
      "---------------------------\n",
      "Epoch 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 87/87 [00:13<00:00,  6.37it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss: 85.24355351924896, Training accuracy : 0.5657225847244263\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:01<00:00,  7.80it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validataion loss: 9.993614375591278, Validation accuracy : 0.5620915293693542\n",
      "Saved at ../models/epoch_3.pth\n",
      "---------------------------\n",
      "Epoch 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 87/87 [00:13<00:00,  6.39it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss: 83.73125845193863, Training accuracy : 0.5846042037010193\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:01<00:00,  8.11it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validataion loss: 9.931407153606415, Validation accuracy : 0.5686274766921997\n",
      "Saved at ../models/epoch_4.pth\n",
      "---------------------------\n",
      "Epoch 6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 87/87 [00:13<00:00,  6.55it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss: 84.33539885282516, Training accuracy : 0.5766158103942871\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:01<00:00,  8.11it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validataion loss: 10.057071208953857, Validation accuracy : 0.5555555820465088\n",
      "Saved at ../models/epoch_5.pth\n",
      "---------------------------\n",
      "Epoch 7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 87/87 [00:13<00:00,  6.49it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss: 84.19107806682587, Training accuracy : 0.5787944793701172\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:01<00:00,  7.95it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validataion loss: 10.056115508079529, Validation accuracy : 0.5555555820465088\n",
      "Saved at ../models/epoch_6.pth\n",
      "---------------------------\n",
      "Epoch 8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 87/87 [00:13<00:00,  6.50it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss: 83.85879647731781, Training accuracy : 0.5824255347251892\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:01<00:00,  7.96it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validataion loss: 10.12123453617096, Validation accuracy : 0.5490196347236633\n",
      "Saved at ../models/epoch_7.pth\n",
      "---------------------------\n",
      "Epoch 9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 87/87 [00:14<00:00,  6.03it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss: 84.11844110488892, Training accuracy : 0.5795207023620605\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:01<00:00,  7.85it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validataion loss: 10.108693897724152, Validation accuracy : 0.5490196347236633\n",
      "Saved at ../models/epoch_8.pth\n",
      "---------------------------\n",
      "Epoch 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 87/87 [00:15<00:00,  5.80it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss: 84.11111134290695, Training accuracy : 0.5802469253540039\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:01<00:00,  7.75it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validataion loss: 9.993442237377167, Validation accuracy : 0.5620915293693542\n",
      "Saved at ../models/epoch_9.pth\n",
      "---------------------------\n",
      "Finished training\n",
      "---------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "train(model, train_dataloader, val_dataloader, loss_fn, optimiser, device, EPOCHS, do_augment=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "0be4c890",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:01<00:00,  7.90it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation loss: 9.743614256381989, Evaluation accuracy : 0.5882353186607361\n",
      "---------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "evaluate(model, test_dataloader, loss_fn, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "c4270da5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import ConfusionMatrixDisplay, confusion_matrix\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "92ad25a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import plot_confusion_matrix\n",
    "\n",
    "mel_spectrogram = torchaudio.transforms.MelSpectrogram(\n",
    "    sample_rate=SAMPLE_RATE,\n",
    "    n_fft=1024,\n",
    "    hop_length=512,\n",
    "    n_mels=128\n",
    ")\n",
    "\n",
    "def evaluate_confusion(model, eval_data_loader, device):\n",
    "    trues = []\n",
    "    preds =[]\n",
    "    for x_batch, y_batch in tqdm(eval_data_loader):\n",
    "        with torch.no_grad():\n",
    "            x_batch, y_batch = x_batch.to(device), y_batch.to(device)\n",
    "\n",
    "            x_batch = x_batch.reshape(-1, x_batch.shape[-1])\n",
    "            #print('x_batch after (batch*channel, time): ', x_batch.shape)\n",
    "            x_batch = mel_spectrogram(x_batch)\n",
    "            #print('x_batch after mel: ', x_batch.shape)\n",
    "            x_batch = x_batch.reshape(x_batch.shape[0], 1, x_batch.shape[-2], x_batch.shape[-1])\n",
    "            \n",
    "            # calculate loss\n",
    "            y_pred = model(x_batch)\n",
    "\n",
    "\n",
    "            trues += torch.clamp(y_batch, max=1)\n",
    "\n",
    "\n",
    "            preds += torch.clamp(torch.argmax(y_pred, axis = 1), max=1)\n",
    "            \n",
    "            \n",
    "    return np.array(trues), np.array(preds)\n",
    "        \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "494fdd01",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:01<00:00,  8.37it/s]\n"
     ]
    }
   ],
   "source": [
    "trues, preds = evaluate_confusion(model, test_dataloader, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "2eba3ddf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<sklearn.metrics._plot.confusion_matrix.ConfusionMatrixDisplay at 0x2a3d86e90>"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "cfm = confusion_matrix(trues, preds)\n",
    "#ConfusionMatrixDisplay(cfm, display_labels=['healthy', 'symptomatic', 'COVID-19']).plot()\n",
    "ConfusionMatrixDisplay(cfm, display_labels=['healthy', 'COVID-19']).plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99a40673",
   "metadata": {},
   "outputs": [],
   "source": [
    "cfm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "16a9e3d4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dtype('int64')"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0264777",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.6 ('.venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "vscode": {
   "interpreter": {
    "hash": "9f480ccb6be28d4b7a840fe3b1174df64fdfbdad14d75b02fbd7e3b419aecfda"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
