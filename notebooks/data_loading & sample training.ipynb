{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "72bf15ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "# import audiomentations\n",
    "# from audiomentations import Compose, AddGaussianNoise, PitchShift\n",
    "# import torch_audiomentations\n",
    "# from torch_audiomentations import Compose, AddGaussianNoise, PitchShift\n",
    "import torchaudio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a6fe837a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adapted from https://github.com/musikalkemist/pytorchforaudio\n",
    "class CoughDataset(Dataset):\n",
    "\n",
    "    def __init__(self,\n",
    "                 annotations_df,\n",
    "                 audio_dir,\n",
    "                 transformation,\n",
    "                 target_sample_rate,\n",
    "                 num_samples,\n",
    "                 device,\n",
    "                 augment=False,\n",
    "                ):\n",
    "        self.annotations = annotations_df\n",
    "        self.audio_dir = audio_dir\n",
    "        self.device = device\n",
    "        self.transformation = transformation.to(self.device)\n",
    "        self.target_sample_rate = target_sample_rate\n",
    "        self.num_samples = num_samples\n",
    "        self.label_dict = {'healthy':0, 'symptomatic':1, 'COVID-19':2}\n",
    "        \n",
    "        self.do_augment = augment\n",
    "#         self.augmentations = Compose(\n",
    "#                 [\n",
    "#                     AddGaussianNoise(min_amplitude=0.01, max_amplitude=0.05, p=0.5),\n",
    "#                     PitchShift(min_semitones=-8, max_semitones=8, p=0.5)\n",
    "#                 ]\n",
    "#         )\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.annotations)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        audio_sample_path = self._get_audio_sample_path(index)\n",
    "        label = self.label_dict[self._get_audio_sample_label(index)]\n",
    "        signal, sr = torchaudio.load(audio_sample_path)\n",
    "        \n",
    "#         if self.do_augment:\n",
    "#             signal = torch.from_numpy(self.augmentations(signal.numpy(), sr))\n",
    "        \n",
    "        signal = signal.to(self.device)\n",
    "        signal = self._resample_if_necessary(signal, sr)\n",
    "        signal = self._mix_down_if_necessary(signal)\n",
    "        signal = self._cut_if_necessary(signal)\n",
    "        signal = self._right_pad_if_necessary(signal)\n",
    "        signal = self.transformation(signal)\n",
    "        \n",
    "        return signal, label\n",
    "\n",
    "    def _cut_if_necessary(self, signal):\n",
    "        if signal.shape[1] > self.num_samples:\n",
    "            signal = signal[:, :self.num_samples]\n",
    "        return signal\n",
    "\n",
    "    def _right_pad_if_necessary(self, signal):\n",
    "        length_signal = signal.shape[1]\n",
    "        if length_signal < self.num_samples:\n",
    "            num_missing_samples = self.num_samples - length_signal\n",
    "            last_dim_padding = (0, num_missing_samples)\n",
    "            signal = torch.nn.functional.pad(signal, last_dim_padding)\n",
    "        return signal\n",
    "\n",
    "    def _resample_if_necessary(self, signal, sr):\n",
    "        if sr != self.target_sample_rate:\n",
    "            resampler = torchaudio.transforms.Resample(sr, self.target_sample_rate)\n",
    "            signal = resampler(signal)\n",
    "        return signal\n",
    "\n",
    "    def _mix_down_if_necessary(self, signal):\n",
    "        if signal.shape[0] > 1:\n",
    "            signal = torch.mean(signal, dim=0, keepdim=True)\n",
    "        return signal\n",
    "\n",
    "    def _get_audio_sample_path(self, index):\n",
    "        path = os.path.join(self.audio_dir, self.annotations.iloc[index, 0])+\".wav\"\n",
    "        return path\n",
    "\n",
    "    def _get_audio_sample_label(self, index):\n",
    "        return self.annotations.iloc[index, 9]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d5a17a8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "AUDIO_DIR = \"../valid_data/\"\n",
    "SAMPLE_RATE = 16000\n",
    "NUM_SAMPLES = SAMPLE_RATE*10\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    device = \"cuda\"\n",
    "else:\n",
    "    device = \"cpu\"\n",
    "# print(f\"Using device {device}\")\n",
    "\n",
    "train_df = pd.read_parquet(os.path.join(AUDIO_DIR, \"train.parquet.gzip\"))\n",
    "val_df = pd.read_parquet(os.path.join(AUDIO_DIR, \"val.parquet.gzip\"))\n",
    "test_df = pd.read_parquet(os.path.join(AUDIO_DIR, \"test.parquet.gzip\"))\n",
    "\n",
    "# print(f\"There are {len(usd)} samples in the dataset.\")\n",
    "# signal, label = usd[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "8045ced4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 128, 313])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[0][0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c258e123",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNNNetwork(torch.nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # 4 conv blocks / flatten / linear / softmax\n",
    "        self.conv1 = nn.Sequential(\n",
    "            nn.Conv2d(\n",
    "                in_channels=1,\n",
    "                out_channels=8,\n",
    "                kernel_size=5,\n",
    "                stride=2,\n",
    "                padding=0\n",
    "            ),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2),\n",
    "            nn.BatchNorm2d(8)\n",
    "        )\n",
    "\n",
    "        self.conv2 = nn.Sequential(\n",
    "            nn.Conv2d(\n",
    "                in_channels=8,\n",
    "                out_channels=12,\n",
    "                kernel_size=3,\n",
    "                stride=1,\n",
    "                padding=0\n",
    "            ),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2),\n",
    "            nn.BatchNorm2d(12)\n",
    "        )\n",
    "        self.conv3 = nn.Sequential(\n",
    "            nn.Conv2d(\n",
    "                in_channels=12,\n",
    "                out_channels=12,\n",
    "                kernel_size=3,\n",
    "                stride=1,\n",
    "                padding=0\n",
    "            ),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2),\n",
    "            nn.BatchNorm2d(12)\n",
    "        )\n",
    "        self.conv4 = nn.Sequential(\n",
    "            nn.Conv2d(\n",
    "                in_channels=12,\n",
    "                out_channels=16,\n",
    "                kernel_size=3,\n",
    "                stride=1,\n",
    "                padding=0\n",
    "            ),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2),\n",
    "            nn.BatchNorm2d(16)\n",
    "        )\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.linear = nn.Linear(224, 3)\n",
    "        self.softmax = nn.Softmax(dim=-1)\n",
    "\n",
    "    def forward(self, input_data):\n",
    "        #nomralization\n",
    "        std = input_data.std()\n",
    "        input_data -= input_data.mean()\n",
    "        input_data /= std\n",
    "        \n",
    "        x = self.conv1(input_data)\n",
    "        x = self.conv2(x)\n",
    "        x = self.conv3(x)\n",
    "        x = self.conv4(x)\n",
    "        x = self.flatten(x)\n",
    "        logits = self.linear(x)\n",
    "        predictions = self.softmax(logits)\n",
    "        return predictions\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "2c406986",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_data_loader(train_data, batch_size):\n",
    "    train_dataloader = DataLoader(train_data, batch_size=batch_size)\n",
    "    return train_dataloader\n",
    "\n",
    "def count_correct(logits, y_true):\n",
    "    y_pred = torch.argmax(logits, axis = 1)\n",
    "    return torch.sum(y_pred==y_true)\n",
    "\n",
    "def train_single_epoch(model, train_data_loader, val_data_loader, loss_fn, optimiser, device):\n",
    "    total_loss = 0.0\n",
    "    correct_pred = 0.0\n",
    "    total_pred = 0\n",
    "    for x_batch, y_batch in tqdm(train_data_loader):\n",
    "        x_batch, y_batch = x_batch.to(device), y_batch.to(device)\n",
    "\n",
    "        # calculate loss\n",
    "        y_pred = model(x_batch)\n",
    "        loss = loss_fn(y_pred, y_batch)\n",
    "        \n",
    "        correct_pred += count_correct(y_pred, y_batch)\n",
    "        total_pred += y_batch.shape[0]\n",
    "\n",
    "        # backpropagate error and update weights\n",
    "        optimiser.zero_grad()\n",
    "        loss.backward()\n",
    "        optimiser.step()\n",
    "        total_loss += loss.item()\n",
    "        \n",
    "    print(f\"Training loss: {total_loss}, Training accuracy : {correct_pred/total_pred}\")\n",
    "    \n",
    "    total_loss = 0.0\n",
    "    correct_pred = 0.0\n",
    "    total_pred = 0\n",
    "    for x_batch, y_batch in tqdm(val_data_loader):\n",
    "        with torch.no_grad():\n",
    "            x_batch, y_batch = x_batch.to(device), y_batch.to(device)\n",
    "            y_pred = model(x_batch)\n",
    "            loss = loss_fn(y_pred, y_batch)\n",
    "            total_loss += loss.item() \n",
    "            \n",
    "        correct_pred += count_correct(y_pred, y_batch)\n",
    "        total_pred += y_batch.shape[0]\n",
    "        \n",
    "    print(f\"Validataion loss: {total_loss}, Validation accuracy : {correct_pred/total_pred}\")\n",
    "\n",
    "    \n",
    "def train(model, train_data_loader, val_data_loader, loss_fn, optimiser, device, epochs):\n",
    "    for i in range(epochs):\n",
    "        print(f\"Epoch {i+1}\")\n",
    "        train_single_epoch(model, train_data_loader, val_data_loader, loss_fn, optimiser, device)\n",
    "        \n",
    "        path = os.path.join(MODEL_FOLDER, f\"epoch_{i}.pth\")\n",
    "        torch.save(model.state_dict(), path)\n",
    "        print(f\"Saved at {path}\")\n",
    "        print(\"---------------------------\")\n",
    "    print(\"Finished training\")\n",
    "    print(\"---------------------------\")\n",
    "    \n",
    "    \n",
    "def evaluate(model, eval_data_loader, loss_fn, device):\n",
    "    print(\"Evaluating model\")\n",
    "    total_loss = 0.0\n",
    "    correct_pred = 0.0\n",
    "    total_pred = 0\n",
    "    for x_batch, y_batch in tqdm(eval_data_loader):\n",
    "        with torch.no_grad():\n",
    "            x_batch, y_batch = x_batch.to(device), y_batch.to(device)\n",
    "\n",
    "            # calculate loss\n",
    "            y_pred = model(x_batch)\n",
    "            loss = loss_fn(y_pred, y_batch)\n",
    "\n",
    "            correct_pred += count_correct(y_pred, y_batch)\n",
    "            total_pred += y_batch.shape[0]\n",
    "\n",
    "            total_loss += loss.item()\n",
    "        \n",
    "    print(f\"Evaluation loss: {total_loss}, Evaluation accuracy : {correct_pred/total_pred}\")\n",
    "    print(\"---------------------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "b393f0aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE =16\n",
    "EPOCHS = 10\n",
    "MODEL_FOLDER = '../models/'\n",
    "\n",
    "mel_spectrogram = torchaudio.transforms.MelSpectrogram(\n",
    "    sample_rate=SAMPLE_RATE,\n",
    "    n_fft=1024,\n",
    "    hop_length=512,\n",
    "    n_mels=128\n",
    ")\n",
    "\n",
    "train_data = CoughDataset(train_df,\n",
    "                        AUDIO_DIR,\n",
    "                        mel_spectrogram,\n",
    "                        SAMPLE_RATE,\n",
    "                        NUM_SAMPLES,\n",
    "                        device)\n",
    "\n",
    "val_data = CoughDataset(val_df,\n",
    "                        AUDIO_DIR,\n",
    "                        mel_spectrogram,\n",
    "                        SAMPLE_RATE,\n",
    "                        NUM_SAMPLES,\n",
    "                        device)\n",
    "\n",
    "test_data = CoughDataset(test_df,\n",
    "                        AUDIO_DIR,\n",
    "                        mel_spectrogram,\n",
    "                        SAMPLE_RATE,\n",
    "                        NUM_SAMPLES,\n",
    "                        device)\n",
    "\n",
    "train_dataloader = create_data_loader(train_data, BATCH_SIZE)\n",
    "val_dataloader = create_data_loader(val_data, BATCH_SIZE)\n",
    "test_dataloader = create_data_loader(val_data, BATCH_SIZE)\n",
    "\n",
    "# construct model and assign it to device\n",
    "model = CNNNetwork().to(device)\n",
    "\n",
    "# initialise loss funtion + optimiser\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimiser = torch.optim.Adam(model.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "849e78e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 686/686 [04:17<00:00,  2.66it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss: 566.4405611753464, Training accuracy : 0.7321656346321106\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 77/77 [00:24<00:00,  3.10it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validataion loss: 62.23724299669266, Validation accuracy : 0.7481542229652405\n",
      "Saved at ../models/epoch_0.pth\n",
      "---------------------------\n",
      "Epoch 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 686/686 [03:56<00:00,  2.90it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss: 551.3338984251022, Training accuracy : 0.748038649559021\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 77/77 [00:23<00:00,  3.33it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validataion loss: 62.21875709295273, Validation accuracy : 0.7481542229652405\n",
      "Saved at ../models/epoch_1.pth\n",
      "---------------------------\n",
      "Epoch 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 686/686 [03:43<00:00,  3.06it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss: 550.9948009848595, Training accuracy : 0.7484948039054871\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 77/77 [00:22<00:00,  3.45it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validataion loss: 62.20021456480026, Validation accuracy : 0.7481542229652405\n",
      "Saved at ../models/epoch_2.pth\n",
      "---------------------------\n",
      "Epoch 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 686/686 [03:29<00:00,  3.27it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss: 550.6316012740135, Training accuracy : 0.7490421533584595\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 77/77 [00:20<00:00,  3.81it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validataion loss: 62.28057438135147, Validation accuracy : 0.7481542229652405\n",
      "Saved at ../models/epoch_3.pth\n",
      "---------------------------\n",
      "Epoch 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 686/686 [03:22<00:00,  3.38it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss: 550.576662003994, Training accuracy : 0.7492246031761169\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 77/77 [00:20<00:00,  3.83it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validataion loss: 62.24333715438843, Validation accuracy : 0.7473338842391968\n",
      "Saved at ../models/epoch_4.pth\n",
      "---------------------------\n",
      "Epoch 6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 686/686 [03:25<00:00,  3.33it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss: 550.0520678758621, Training accuracy : 0.7499544024467468\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 77/77 [00:24<00:00,  3.20it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validataion loss: 62.2738618850708, Validation accuracy : 0.7481542229652405\n",
      "Saved at ../models/epoch_5.pth\n",
      "---------------------------\n",
      "Epoch 7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 686/686 [04:04<00:00,  2.80it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss: 550.184578359127, Training accuracy : 0.7501368522644043\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 77/77 [00:24<00:00,  3.18it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validataion loss: 62.26168352365494, Validation accuracy : 0.7465135455131531\n",
      "Saved at ../models/epoch_6.pth\n",
      "---------------------------\n",
      "Epoch 8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 686/686 [04:05<00:00,  2.79it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss: 550.4087800383568, Training accuracy : 0.7492246031761169\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 77/77 [00:24<00:00,  3.16it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validataion loss: 62.30344915390015, Validation accuracy : 0.7456932067871094\n",
      "Saved at ../models/epoch_7.pth\n",
      "---------------------------\n",
      "Epoch 9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 686/686 [04:00<00:00,  2.86it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss: 550.6456421613693, Training accuracy : 0.7489508986473083\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 77/77 [00:24<00:00,  3.10it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validataion loss: 62.22816425561905, Validation accuracy : 0.7473338842391968\n",
      "Saved at ../models/epoch_8.pth\n",
      "---------------------------\n",
      "Epoch 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 686/686 [04:52<00:00,  2.35it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss: 549.7948590517044, Training accuracy : 0.7501368522644043\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 77/77 [00:25<00:00,  3.04it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validataion loss: 62.18648952245712, Validation accuracy : 0.7481542229652405\n",
      "Saved at ../models/epoch_9.pth\n",
      "---------------------------\n",
      "Finished training\n",
      "---------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "train(model, train_dataloader, val_dataloader, loss_fn, optimiser, device, EPOCHS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "0be4c890",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 77/77 [00:24<00:00,  3.15it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation loss: 82.63644808530807, Evaluation accuracy : 0.38392123579978943\n",
      "---------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "evaluate(model, test_dataloader, loss_fn, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "c4270da5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import ConfusionMatrixDisplay, confusion_matrix\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "92ad25a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import plot_confusion_matrix\n",
    "\n",
    "def evaluate_confusion(model, eval_data_loader, device):\n",
    "    trues = []\n",
    "    preds =[]\n",
    "    for x_batch, y_batch in tqdm(eval_data_loader):\n",
    "        with torch.no_grad():\n",
    "            x_batch, y_batch = x_batch.to(device), y_batch.to(device)\n",
    "\n",
    "            # calculate loss\n",
    "            y_pred = model(x_batch)\n",
    "            trues += y_batch\n",
    "            preds += torch.argmax(y_pred, axis = 1)\n",
    "            \n",
    "    return np.array(trues), np.array(preds)\n",
    "        \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "494fdd01",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 77/77 [00:23<00:00,  3.28it/s]\n"
     ]
    }
   ],
   "source": [
    "trues, preds = evaluate_confusion(model, test_dataloader, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "2eba3ddf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<sklearn.metrics._plot.confusion_matrix.ConfusionMatrixDisplay at 0x7f80d3d6fe50>"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "cfm = confusion_matrix(trues, preds)\n",
    "ConfusionMatrixDisplay(cfm, display_labels=['healthy', 'symptomatic', 'COVID-19']).plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "99a40673",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'cfm' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [64]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mcfm\u001b[49m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'cfm' is not defined"
     ]
    }
   ],
   "source": [
    "cfm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "16a9e3d4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dtype('int64')"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0264777",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
