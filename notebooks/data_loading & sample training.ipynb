{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "72bf15ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import audiomentations\n",
    "#from audiomentations import Compose, AddGaussianNoise, PitchShift\n",
    "#import torch_audiomentations\n",
    "from torch_audiomentations import Compose, PitchShift, TimeInversion\n",
    "import tensorboard\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import torchaudio\n",
    "import numpy as np\n",
    "import sys\n",
    "\n",
    "sys.path.insert(0, '../src')\n",
    "from segmentation import segment_cough\n",
    "\n",
    "\n",
    "WRITER_PATH =\"../logs/CNN\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "a6fe837a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adapted from https://github.com/musikalkemist/pytorchforaudio\n",
    "class CoughDataset(Dataset):\n",
    "\n",
    "    def __init__(self,\n",
    "                 annotations_df,\n",
    "                 audio_dir,\n",
    "                 transformation,\n",
    "                 target_sample_rate,\n",
    "                 num_samples,\n",
    "                 device,\n",
    "                #  segment=False,\n",
    "                 augment=False,\n",
    "                ):\n",
    "        self.annotations = annotations_df\n",
    "        self.audio_dir = audio_dir\n",
    "        self.device = device\n",
    "        self.transformation = transformation.to(self.device)\n",
    "        self.target_sample_rate = target_sample_rate\n",
    "        self.num_samples = num_samples\n",
    "        self.label_dict = {'healthy':0, 'symptomatic':1, 'COVID-19':2}\n",
    "        self.label_weights = self._calculate_weights(annotations_df)\n",
    "        \n",
    "        # self.do_segment = segment\n",
    "\n",
    "        # self.segmentation = segment_cough\n",
    "\n",
    "        self.do_augment = augment\n",
    "        # self.augmentations = Compose(\n",
    "        #         [\n",
    "        #             AddGaussianNoise(min_amplitude=0.01, max_amplitude=0.05, p=0.5),\n",
    "        #             PitchShift(min_semitones=-8, max_semitones=8, p=0.5)\n",
    "        #         ]\n",
    "        # )\n",
    "        self.augmentations = Compose(\n",
    "            transforms=[\n",
    "                PitchShift(\n",
    "                    mode = \"per_example\",\n",
    "                    p=0.5,\n",
    "                    sample_rate=self.target_sample_rate\n",
    "                    ),\n",
    "                TimeInversion(\n",
    "                    mode = \"per_example\",\n",
    "                    p=0.5,\n",
    "                ),\n",
    "            ]\n",
    "        )\n",
    "\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.annotations)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        audio_sample_path = self._get_audio_sample_path(index)\n",
    "        label = self.label_dict[self._get_audio_sample_label(index)]\n",
    "        signal, sr = torchaudio.load(audio_sample_path)\n",
    "\n",
    "        # if self.do_segment:\n",
    "        #     segments, segment_mask = self.segmentation(signal.numpy()[0], float(sr))\n",
    "        #     #print('segment length: ', len(segments))\n",
    "        #     if len(segments) > 0:\n",
    "        #         signal = torch.tensor(segments[0])\n",
    "        #         signal = signal.unsqueeze(0)\n",
    "        #     else:\n",
    "        #         signal = signal\n",
    "        #     #print('signal in self.do_segment: ', signal)\n",
    "\n",
    "        \n",
    "        signal = signal.to(self.device)\n",
    "        signal = self._resample_if_necessary(signal, sr)\n",
    "        signal = self._mix_down_if_necessary(signal)\n",
    "        signal = self._cut_if_necessary(signal)\n",
    "        signal = self._right_pad_if_necessary(signal)\n",
    "\n",
    "        if self.do_augment:\n",
    "            #signal = torch.from_numpy(self.augmentations(signal.numpy(), sr))\n",
    "            # print('signal shape: ', signal.shape)\n",
    "            # print('sample rate: ', sr)\n",
    "            # print('target sample rate: ', self.target_sample_rate)\n",
    "            # add a 1 in front of the signal array\n",
    "            signal = signal.unsqueeze(0)\n",
    "            signal = self.augmentations(signal, self.target_sample_rate)\n",
    "            # remove the 1 in front of the signal array\n",
    "            signal = signal.squeeze(0)\n",
    "        \n",
    "\n",
    "        signal = self.transformation(signal)\n",
    "\n",
    "        \n",
    "        return signal, label\n",
    "\n",
    "    def _cut_if_necessary(self, signal):\n",
    "        if signal.shape[1] > self.num_samples:\n",
    "            signal = signal[:, :self.num_samples]\n",
    "        return signal\n",
    "\n",
    "    def _right_pad_if_necessary(self, signal):\n",
    "        length_signal = signal.shape[1]\n",
    "        if length_signal < self.num_samples:\n",
    "            num_missing_samples = self.num_samples - length_signal\n",
    "            last_dim_padding = (0, num_missing_samples)\n",
    "            signal = torch.nn.functional.pad(signal, last_dim_padding)\n",
    "        return signal\n",
    "\n",
    "    def _resample_if_necessary(self, signal, sr):\n",
    "        # print('resampled signal')\n",
    "        if sr != self.target_sample_rate:\n",
    "            resampler = torchaudio.transforms.Resample(sr, self.target_sample_rate)\n",
    "            signal = resampler(signal)\n",
    "        return signal\n",
    "\n",
    "    def _mix_down_if_necessary(self, signal):\n",
    "        if signal.shape[0] > 1:\n",
    "            signal = torch.mean(signal, dim=0, keepdim=True)\n",
    "        return signal\n",
    "\n",
    "    def _get_audio_sample_path(self, index):\n",
    "        path = os.path.join(self.audio_dir, self.annotations.iloc[index, 0])+\".wav\"\n",
    "        return path\n",
    "\n",
    "    def _get_audio_sample_label(self, index):\n",
    "        return self.annotations.iloc[index, 9]\n",
    "\n",
    "    def _calculate_weights(self, annotation_df):\n",
    "        counts = annotation_df[\"status\"].value_counts()\n",
    "        total = len(annotation_df)\n",
    "        weights = (1-(counts/total))\n",
    "        weights /= weights.sum()\n",
    "        return torch.FloatTensor(weights)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "d5a17a8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "AUDIO_DIR = \"../valid_data/\"\n",
    "SAMPLE_RATE = 16000\n",
    "NUM_SAMPLES = SAMPLE_RATE*10\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    device = \"cuda\"\n",
    "else:\n",
    "    device = \"cpu\"\n",
    "# print(f\"Using device {device}\")\n",
    "\n",
    "# train_df = pd.read_parquet(os.path.join(AUDIO_DIR, \"train.parquet.gzip\"))\n",
    "# val_df = pd.read_parquet(os.path.join(AUDIO_DIR, \"val.parquet.gzip\"))\n",
    "# test_df = pd.read_parquet(os.path.join(AUDIO_DIR, \"test.parquet.gzip\"))\n",
    "\n",
    "# train_df = pd.read_parquet(os.path.join(AUDIO_DIR, \"train_edited.parquet.gzip\"))\n",
    "# val_df = pd.read_parquet(os.path.join(AUDIO_DIR, \"val_edited.parquet.gzip\"))\n",
    "# test_df = pd.read_parquet(os.path.join(AUDIO_DIR, \"test_edited.parquet.gzip\"))\n",
    "\n",
    "train_df = pd.read_parquet(os.path.join(AUDIO_DIR, \"train_balanced.parquet.gzip\"))\n",
    "val_df = pd.read_parquet(os.path.join(AUDIO_DIR, \"val_balanced.parquet.gzip\"))\n",
    "test_df = pd.read_parquet(os.path.join(AUDIO_DIR, \"test_balanced.parquet.gzip\"))\n",
    "\n",
    "\n",
    "# print(f\"There are {len(usd)} samples in the dataset.\")\n",
    "# signal, label = usd[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "c258e123",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNNNetwork(torch.nn.Module):\n",
    "\n",
    "    def __init__(self, drop_p=0.2):\n",
    "        super().__init__()\n",
    "        # 4 conv blocks / flatten / linear / softmax\n",
    "        self.conv1 = nn.Sequential(\n",
    "            nn.Conv2d(\n",
    "                in_channels=1,\n",
    "                out_channels=16,\n",
    "                kernel_size=3,\n",
    "                stride=2,\n",
    "                padding=0\n",
    "            ),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2),\n",
    "            nn.BatchNorm2d(16),\n",
    "            nn.Dropout(p=drop_p)\n",
    "\n",
    "        )\n",
    "\n",
    "        self.conv2 = nn.Sequential(\n",
    "            nn.Conv2d(\n",
    "                in_channels=16,\n",
    "                out_channels=32,\n",
    "                kernel_size=3,\n",
    "                stride=2,\n",
    "                padding=0\n",
    "            ),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=1),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.Dropout(p=drop_p)\n",
    "        )\n",
    "        self.conv3 = nn.Sequential(\n",
    "            nn.Conv2d(\n",
    "                in_channels=32,\n",
    "                out_channels=64,\n",
    "                kernel_size=3,\n",
    "                stride=1,\n",
    "                padding=0\n",
    "            ),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=1),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.Dropout(p=drop_p)\n",
    "        )\n",
    "        self.conv4 = nn.Sequential(\n",
    "            nn.Conv2d(\n",
    "                in_channels=64,\n",
    "                out_channels=128,\n",
    "                kernel_size=3,\n",
    "                stride=1,\n",
    "                padding=0\n",
    "            ),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=1),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.Dropout(p=drop_p)\n",
    "        )\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.linear = nn.Linear(31744, 3)\n",
    "        self.softmax = nn.Softmax(dim=-1)\n",
    "\n",
    "    def forward(self, input_data):\n",
    "        #nomralization\n",
    "        std = input_data.std()\n",
    "        input_data -= input_data.mean()\n",
    "        input_data /= std\n",
    "        \n",
    "        x = self.conv1(input_data)\n",
    "        x = self.conv2(x)\n",
    "        x = self.conv3(x)\n",
    "        x = self.conv4(x)\n",
    "        x = self.flatten(x)\n",
    "        logits = self.linear(x)\n",
    "        predictions = self.softmax(logits)\n",
    "        return predictions\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "2c406986",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_data_loader(train_data, batch_size):\n",
    "    train_dataloader = DataLoader(train_data, batch_size=batch_size)\n",
    "    print('train_dataloader finished: ', train_dataloader)\n",
    "    return train_dataloader\n",
    "\n",
    "def count_correct(logits, y_true):\n",
    "    y_pred = torch.argmax(logits, axis = 1)\n",
    "    return torch.sum(y_pred==y_true)\n",
    "\n",
    "def train_single_epoch(model, train_data_loader, val_data_loader, loss_fn, optimiser, device):\n",
    "    total_loss = 0.0\n",
    "    correct_pred = 0.0\n",
    "    total_pred = 0\n",
    "    for x_batch, y_batch in tqdm(train_data_loader):\n",
    "        x_batch, y_batch = x_batch.to(device), y_batch.to(device)\n",
    "\n",
    "        # calculate loss\n",
    "        y_pred = model(x_batch)\n",
    "        loss = loss_fn(y_pred, y_batch)\n",
    "        \n",
    "        correct_pred += count_correct(y_pred, y_batch)\n",
    "        total_pred += y_batch.shape[0]\n",
    "\n",
    "        # backpropagate error and update weights\n",
    "        optimiser.zero_grad()\n",
    "        loss.backward()\n",
    "        optimiser.step()\n",
    "        total_loss += loss.item()\n",
    "        \n",
    "    print(f\"Training loss: {total_loss}, Training accuracy : {correct_pred/total_pred}\")\n",
    "    train_loss, train_acc = total_loss, correct_pred/total_pred\n",
    "    \n",
    "    total_loss = 0.0\n",
    "    correct_pred = 0.0\n",
    "    total_pred = 0\n",
    "    for x_batch, y_batch in tqdm(val_data_loader):\n",
    "        with torch.no_grad():\n",
    "            x_batch, y_batch = x_batch.to(device), y_batch.to(device)\n",
    "            y_pred = model(x_batch)\n",
    "            loss = loss_fn(y_pred, y_batch)\n",
    "            total_loss += loss.item() \n",
    "            \n",
    "        correct_pred += count_correct(y_pred, y_batch)\n",
    "        total_pred += y_batch.shape[0]\n",
    "        \n",
    "    print(f\"Validataion loss: {total_loss}, Validation accuracy : {correct_pred/total_pred}\")\n",
    "    val_loss, val_acc = total_loss, correct_pred/total_pred\n",
    "\n",
    "    return train_loss, train_acc, val_loss, val_acc\n",
    "\n",
    "    \n",
    "def train(model, train_data_loader, val_data_loader, loss_fn, optimiser, device, epochs):\n",
    "    writer = SummaryWriter(WRITER_PATH)\n",
    "    for i in range(epochs):\n",
    "        print(f\"Epoch {i+1}\")\n",
    "        train_loss, train_acc, val_loss, val_acc = train_single_epoch(model, train_data_loader, val_data_loader, loss_fn, optimiser, device)\n",
    "        writer.add_scalar(\"train/accuracy\", train_acc, i)\n",
    "        writer.add_scalar(\"train/loss\", train_loss, i)\n",
    "        writer.add_scalar(\"validation/accuracy\", val_acc, i)\n",
    "        writer.add_scalar(\"validation/loss\", val_loss, i)\n",
    "\n",
    "        \n",
    "        path = os.path.join(MODEL_FOLDER, f\"epoch_{i}.pth\")\n",
    "        torch.save(model.state_dict(), path)\n",
    "        print(f\"Saved at {path}\")\n",
    "        print(\"---------------------------\")\n",
    "    print(\"Finished training\")\n",
    "    print(\"---------------------------\")\n",
    "    \n",
    "    \n",
    "def evaluate(model, eval_data_loader, loss_fn, device):\n",
    "    print(\"Evaluating model\")\n",
    "    total_loss = 0.0\n",
    "    correct_pred = 0.0\n",
    "    total_pred = 0\n",
    "    for x_batch, y_batch in tqdm(eval_data_loader):\n",
    "        with torch.no_grad():\n",
    "            x_batch, y_batch = x_batch.to(device), y_batch.to(device)\n",
    "\n",
    "            # calculate loss\n",
    "            y_pred = model(x_batch)\n",
    "            loss = loss_fn(y_pred, y_batch)\n",
    "\n",
    "            correct_pred += count_correct(y_pred, y_batch)\n",
    "            total_pred += y_batch.shape[0]\n",
    "\n",
    "            total_loss += loss.item()\n",
    "        \n",
    "    print(f\"Evaluation loss: {total_loss}, Evaluation accuracy : {correct_pred/total_pred}\")\n",
    "    print(\"---------------------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "b393f0aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train data:  <__main__.CoughDataset object at 0x2a5e97c70>\n",
      "train_dataloader finished:  <torch.utils.data.dataloader.DataLoader object at 0x2a5e99120>\n",
      "train_dataloader finished:  <torch.utils.data.dataloader.DataLoader object at 0x2a5e9a620>\n",
      "train_dataloader finished:  <torch.utils.data.dataloader.DataLoader object at 0x2a5e99720>\n"
     ]
    }
   ],
   "source": [
    "BATCH_SIZE =16\n",
    "EPOCHS = 50\n",
    "MODEL_FOLDER = '../models/'\n",
    "\n",
    "mel_spectrogram = torchaudio.transforms.MelSpectrogram(\n",
    "    sample_rate=SAMPLE_RATE,\n",
    "    n_fft=1024,\n",
    "    hop_length=512,\n",
    "    n_mels=128\n",
    ")\n",
    "\n",
    "train_data = CoughDataset(train_df,\n",
    "                        AUDIO_DIR,\n",
    "                        mel_spectrogram,\n",
    "                        SAMPLE_RATE,\n",
    "                        NUM_SAMPLES,\n",
    "                        device,\n",
    "                        )\n",
    "print('train data: ', train_data)\n",
    "\n",
    "val_data = CoughDataset(val_df,\n",
    "                        AUDIO_DIR,\n",
    "                        mel_spectrogram,\n",
    "                        SAMPLE_RATE,\n",
    "                        NUM_SAMPLES,\n",
    "                        device)\n",
    "\n",
    "test_data = CoughDataset(test_df,\n",
    "                        AUDIO_DIR,\n",
    "                        mel_spectrogram,\n",
    "                        SAMPLE_RATE,\n",
    "                        NUM_SAMPLES,\n",
    "                        device)\n",
    "\n",
    "train_dataloader = create_data_loader(train_data, BATCH_SIZE)\n",
    "val_dataloader = create_data_loader(val_data, BATCH_SIZE)\n",
    "test_dataloader = create_data_loader(val_data, BATCH_SIZE)\n",
    "\n",
    "# construct model and assign it to device\n",
    "model = CNNNetwork().to(device)\n",
    "\n",
    "# initialise loss funtion + optimiser\n",
    "#loss_fn = nn.CrossEntropyLoss(weight=train_data.label_weights)\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimiser = torch.optim.Adam(model.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "849e78e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 361/361 [05:38<00:00,  1.07it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss: 248.94485533237457, Training accuracy : 0.8602635264396667\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 41/41 [00:06<00:00,  6.60it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validataion loss: 27.654162287712097, Validation accuracy : 0.873634934425354\n",
      "Saved at ../models/epoch_0.pth\n",
      "---------------------------\n",
      "Epoch 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 361/361 [05:41<00:00,  1.06it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss: 242.4827030301094, Training accuracy : 0.8796809911727905\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 41/41 [00:05<00:00,  7.45it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validataion loss: 27.920560657978058, Validation accuracy : 0.8907956480979919\n",
      "Saved at ../models/epoch_1.pth\n",
      "---------------------------\n",
      "Epoch 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 361/361 [05:28<00:00,  1.10it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss: 236.76550006866455, Training accuracy : 0.8956310749053955\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 41/41 [00:05<00:00,  7.12it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validataion loss: 27.79117202758789, Validation accuracy : 0.8939157724380493\n",
      "Saved at ../models/epoch_2.pth\n",
      "---------------------------\n",
      "Epoch 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 361/361 [05:25<00:00,  1.11it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss: 235.50523167848587, Training accuracy : 0.8992718458175659\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 41/41 [00:05<00:00,  7.25it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validataion loss: 27.79673081636429, Validation accuracy : 0.8939157724380493\n",
      "Saved at ../models/epoch_3.pth\n",
      "---------------------------\n",
      "Epoch 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 361/361 [05:45<00:00,  1.05it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss: 235.7590863108635, Training accuracy : 0.8985783457756042\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 41/41 [00:06<00:00,  6.82it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validataion loss: 27.75628972053528, Validation accuracy : 0.8954758048057556\n",
      "Saved at ../models/epoch_4.pth\n",
      "---------------------------\n",
      "Epoch 6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 361/361 [05:43<00:00,  1.05it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss: 237.03465408086777, Training accuracy : 0.8949375748634338\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 41/41 [00:05<00:00,  7.13it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validataion loss: 27.7342312335968, Validation accuracy : 0.8954758048057556\n",
      "Saved at ../models/epoch_5.pth\n",
      "---------------------------\n",
      "Epoch 7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 361/361 [05:32<00:00,  1.08it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss: 236.29355019330978, Training accuracy : 0.8970180153846741\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 41/41 [00:05<00:00,  7.04it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validataion loss: 27.796731293201447, Validation accuracy : 0.8939157724380493\n",
      "Saved at ../models/epoch_6.pth\n",
      "---------------------------\n",
      "Epoch 8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 361/361 [05:28<00:00,  1.10it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss: 236.41378819942474, Training accuracy : 0.8966712951660156\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 41/41 [00:05<00:00,  7.17it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validataion loss: 27.60579687356949, Validation accuracy : 0.898595929145813\n",
      "Saved at ../models/epoch_7.pth\n",
      "---------------------------\n",
      "Epoch 9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 361/361 [05:29<00:00,  1.09it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss: 235.19065523147583, Training accuracy : 0.9001386761665344\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 41/41 [00:05<00:00,  7.29it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validataion loss: 27.60936725139618, Validation accuracy : 0.898595929145813\n",
      "Saved at ../models/epoch_8.pth\n",
      "---------------------------\n",
      "Epoch 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 361/361 [05:34<00:00,  1.08it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss: 235.25652247667313, Training accuracy : 0.8999653458595276\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 41/41 [00:05<00:00,  6.83it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validataion loss: 27.54673147201538, Validation accuracy : 0.9001560211181641\n",
      "Saved at ../models/epoch_9.pth\n",
      "---------------------------\n",
      "Epoch 11\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 85%|████████▌ | 308/361 [05:09<00:53,  1.01s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[66], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m train(model, train_dataloader, val_dataloader, loss_fn, optimiser, device, EPOCHS)\n",
      "Cell \u001b[0;32mIn[64], line 51\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(model, train_data_loader, val_data_loader, loss_fn, optimiser, device, epochs)\u001b[0m\n\u001b[1;32m     49\u001b[0m \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(epochs):\n\u001b[1;32m     50\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mEpoch \u001b[39m\u001b[39m{\u001b[39;00mi\u001b[39m+\u001b[39m\u001b[39m1\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n\u001b[0;32m---> 51\u001b[0m     train_single_epoch(model, train_data_loader, val_data_loader, loss_fn, optimiser, device)\n\u001b[1;32m     53\u001b[0m     path \u001b[39m=\u001b[39m os\u001b[39m.\u001b[39mpath\u001b[39m.\u001b[39mjoin(MODEL_FOLDER, \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mepoch_\u001b[39m\u001b[39m{\u001b[39;00mi\u001b[39m}\u001b[39;00m\u001b[39m.pth\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m     54\u001b[0m     torch\u001b[39m.\u001b[39msave(model\u001b[39m.\u001b[39mstate_dict(), path)\n",
      "Cell \u001b[0;32mIn[64], line 14\u001b[0m, in \u001b[0;36mtrain_single_epoch\u001b[0;34m(model, train_data_loader, val_data_loader, loss_fn, optimiser, device)\u001b[0m\n\u001b[1;32m     12\u001b[0m correct_pred \u001b[39m=\u001b[39m \u001b[39m0.0\u001b[39m\n\u001b[1;32m     13\u001b[0m total_pred \u001b[39m=\u001b[39m \u001b[39m0\u001b[39m\n\u001b[0;32m---> 14\u001b[0m \u001b[39mfor\u001b[39;00m x_batch, y_batch \u001b[39min\u001b[39;00m tqdm(train_data_loader):\n\u001b[1;32m     15\u001b[0m     x_batch, y_batch \u001b[39m=\u001b[39m x_batch\u001b[39m.\u001b[39mto(device), y_batch\u001b[39m.\u001b[39mto(device)\n\u001b[1;32m     17\u001b[0m     \u001b[39m# calculate loss\u001b[39;00m\n",
      "File \u001b[0;32m~/Documents/cs230/coughvid/.venv/lib/python3.10/site-packages/tqdm/std.py:1195\u001b[0m, in \u001b[0;36mtqdm.__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1192\u001b[0m time \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_time\n\u001b[1;32m   1194\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m-> 1195\u001b[0m     \u001b[39mfor\u001b[39;00m obj \u001b[39min\u001b[39;00m iterable:\n\u001b[1;32m   1196\u001b[0m         \u001b[39myield\u001b[39;00m obj\n\u001b[1;32m   1197\u001b[0m         \u001b[39m# Update and possibly print the progressbar.\u001b[39;00m\n\u001b[1;32m   1198\u001b[0m         \u001b[39m# Note: does not call self.update(1) for speed optimisation.\u001b[39;00m\n",
      "File \u001b[0;32m~/Documents/cs230/coughvid/.venv/lib/python3.10/site-packages/torch/utils/data/dataloader.py:628\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    625\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_sampler_iter \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    626\u001b[0m     \u001b[39m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    627\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_reset()  \u001b[39m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 628\u001b[0m data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_next_data()\n\u001b[1;32m    629\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[1;32m    630\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_dataset_kind \u001b[39m==\u001b[39m _DatasetKind\u001b[39m.\u001b[39mIterable \u001b[39mand\u001b[39;00m \\\n\u001b[1;32m    631\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m \\\n\u001b[1;32m    632\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m>\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[0;32m~/Documents/cs230/coughvid/.venv/lib/python3.10/site-packages/torch/utils/data/dataloader.py:671\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    669\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_next_data\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[1;32m    670\u001b[0m     index \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_next_index()  \u001b[39m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m--> 671\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_dataset_fetcher\u001b[39m.\u001b[39;49mfetch(index)  \u001b[39m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m    672\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_pin_memory:\n\u001b[1;32m    673\u001b[0m         data \u001b[39m=\u001b[39m _utils\u001b[39m.\u001b[39mpin_memory\u001b[39m.\u001b[39mpin_memory(data, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[0;32m~/Documents/cs230/coughvid/.venv/lib/python3.10/site-packages/torch/utils/data/_utils/fetch.py:58\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     56\u001b[0m         data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset\u001b[39m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[1;32m     57\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m---> 58\u001b[0m         data \u001b[39m=\u001b[39m [\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset[idx] \u001b[39mfor\u001b[39;00m idx \u001b[39min\u001b[39;00m possibly_batched_index]\n\u001b[1;32m     59\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m     60\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[0;32m~/Documents/cs230/coughvid/.venv/lib/python3.10/site-packages/torch/utils/data/_utils/fetch.py:58\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     56\u001b[0m         data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset\u001b[39m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[1;32m     57\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m---> 58\u001b[0m         data \u001b[39m=\u001b[39m [\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdataset[idx] \u001b[39mfor\u001b[39;00m idx \u001b[39min\u001b[39;00m possibly_batched_index]\n\u001b[1;32m     59\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m     60\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset[possibly_batched_index]\n",
      "Cell \u001b[0;32mIn[61], line 55\u001b[0m, in \u001b[0;36mCoughDataset.__getitem__\u001b[0;34m(self, index)\u001b[0m\n\u001b[1;32m     53\u001b[0m audio_sample_path \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_get_audio_sample_path(index)\n\u001b[1;32m     54\u001b[0m label \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlabel_dict[\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_get_audio_sample_label(index)]\n\u001b[0;32m---> 55\u001b[0m signal, sr \u001b[39m=\u001b[39m torchaudio\u001b[39m.\u001b[39;49mload(audio_sample_path)\n\u001b[1;32m     57\u001b[0m \u001b[39m# if self.do_segment:\u001b[39;00m\n\u001b[1;32m     58\u001b[0m \u001b[39m#     segments, segment_mask = self.segmentation(signal.numpy()[0], float(sr))\u001b[39;00m\n\u001b[1;32m     59\u001b[0m \u001b[39m#     #print('segment length: ', len(segments))\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     64\u001b[0m \u001b[39m#         signal = signal\u001b[39;00m\n\u001b[1;32m     65\u001b[0m \u001b[39m#     #print('signal in self.do_segment: ', signal)\u001b[39;00m\n\u001b[1;32m     68\u001b[0m signal \u001b[39m=\u001b[39m signal\u001b[39m.\u001b[39mto(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdevice)\n",
      "File \u001b[0;32m~/Documents/cs230/coughvid/.venv/lib/python3.10/site-packages/torchaudio/backend/soundfile_backend.py:214\u001b[0m, in \u001b[0;36mload\u001b[0;34m(filepath, frame_offset, num_frames, normalize, channels_first, format)\u001b[0m\n\u001b[1;32m    211\u001b[0m         dtype \u001b[39m=\u001b[39m _SUBTYPE2DTYPE[file_\u001b[39m.\u001b[39msubtype]\n\u001b[1;32m    213\u001b[0m     frames \u001b[39m=\u001b[39m file_\u001b[39m.\u001b[39m_prepare_read(frame_offset, \u001b[39mNone\u001b[39;00m, num_frames)\n\u001b[0;32m--> 214\u001b[0m     waveform \u001b[39m=\u001b[39m file_\u001b[39m.\u001b[39;49mread(frames, dtype, always_2d\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n\u001b[1;32m    215\u001b[0m     sample_rate \u001b[39m=\u001b[39m file_\u001b[39m.\u001b[39msamplerate\n\u001b[1;32m    217\u001b[0m waveform \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mfrom_numpy(waveform)\n",
      "File \u001b[0;32m~/Documents/cs230/coughvid/.venv/lib/python3.10/site-packages/soundfile.py:892\u001b[0m, in \u001b[0;36mSoundFile.read\u001b[0;34m(self, frames, dtype, always_2d, fill_value, out)\u001b[0m\n\u001b[1;32m    890\u001b[0m     \u001b[39mif\u001b[39;00m frames \u001b[39m<\u001b[39m \u001b[39m0\u001b[39m \u001b[39mor\u001b[39;00m frames \u001b[39m>\u001b[39m \u001b[39mlen\u001b[39m(out):\n\u001b[1;32m    891\u001b[0m         frames \u001b[39m=\u001b[39m \u001b[39mlen\u001b[39m(out)\n\u001b[0;32m--> 892\u001b[0m frames \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_array_io(\u001b[39m'\u001b[39;49m\u001b[39mread\u001b[39;49m\u001b[39m'\u001b[39;49m, out, frames)\n\u001b[1;32m    893\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(out) \u001b[39m>\u001b[39m frames:\n\u001b[1;32m    894\u001b[0m     \u001b[39mif\u001b[39;00m fill_value \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/Documents/cs230/coughvid/.venv/lib/python3.10/site-packages/soundfile.py:1341\u001b[0m, in \u001b[0;36mSoundFile._array_io\u001b[0;34m(self, action, array, frames)\u001b[0m\n\u001b[1;32m   1339\u001b[0m \u001b[39massert\u001b[39;00m array\u001b[39m.\u001b[39mdtype\u001b[39m.\u001b[39mitemsize \u001b[39m==\u001b[39m _ffi\u001b[39m.\u001b[39msizeof(ctype)\n\u001b[1;32m   1340\u001b[0m cdata \u001b[39m=\u001b[39m _ffi\u001b[39m.\u001b[39mcast(ctype \u001b[39m+\u001b[39m \u001b[39m'\u001b[39m\u001b[39m*\u001b[39m\u001b[39m'\u001b[39m, array\u001b[39m.\u001b[39m__array_interface__[\u001b[39m'\u001b[39m\u001b[39mdata\u001b[39m\u001b[39m'\u001b[39m][\u001b[39m0\u001b[39m])\n\u001b[0;32m-> 1341\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_cdata_io(action, cdata, ctype, frames)\n",
      "File \u001b[0;32m~/Documents/cs230/coughvid/.venv/lib/python3.10/site-packages/soundfile.py:1350\u001b[0m, in \u001b[0;36mSoundFile._cdata_io\u001b[0;34m(self, action, data, ctype, frames)\u001b[0m\n\u001b[1;32m   1348\u001b[0m     curr \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtell()\n\u001b[1;32m   1349\u001b[0m func \u001b[39m=\u001b[39m \u001b[39mgetattr\u001b[39m(_snd, \u001b[39m'\u001b[39m\u001b[39msf_\u001b[39m\u001b[39m'\u001b[39m \u001b[39m+\u001b[39m action \u001b[39m+\u001b[39m \u001b[39m'\u001b[39m\u001b[39mf_\u001b[39m\u001b[39m'\u001b[39m \u001b[39m+\u001b[39m ctype)\n\u001b[0;32m-> 1350\u001b[0m frames \u001b[39m=\u001b[39m func(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_file, data, frames)\n\u001b[1;32m   1351\u001b[0m _error_check(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_errorcode)\n\u001b[1;32m   1352\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mseekable():\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "train(model, train_dataloader, val_dataloader, loss_fn, optimiser, device, EPOCHS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "0be4c890",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 41/41 [00:07<00:00,  5.48it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation loss: 27.61043083667755, Evaluation accuracy : 0.898595929145813\n",
      "---------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "evaluate(model, test_dataloader, loss_fn, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "c4270da5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import ConfusionMatrixDisplay, confusion_matrix\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "92ad25a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import plot_confusion_matrix\n",
    "\n",
    "def evaluate_confusion(model, eval_data_loader, device):\n",
    "    trues = []\n",
    "    preds =[]\n",
    "    for x_batch, y_batch in tqdm(eval_data_loader):\n",
    "        with torch.no_grad():\n",
    "            x_batch, y_batch = x_batch.to(device), y_batch.to(device)\n",
    "\n",
    "            # calculate loss\n",
    "            y_pred = model(x_batch)\n",
    "\n",
    "\n",
    "            trues += torch.clamp(y_batch, max=1)\n",
    "\n",
    "\n",
    "            preds += torch.clamp(torch.argmax(y_pred, axis = 1), max=1)\n",
    "            \n",
    "            \n",
    "    return np.array(trues), np.array(preds)\n",
    "        \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "494fdd01",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 41/41 [00:07<00:00,  5.30it/s]\n"
     ]
    }
   ],
   "source": [
    "trues, preds = evaluate_confusion(model, test_dataloader, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "2eba3ddf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<sklearn.metrics._plot.confusion_matrix.ConfusionMatrixDisplay at 0x29d9fc220>"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "cfm = confusion_matrix(trues, preds)\n",
    "#ConfusionMatrixDisplay(cfm, display_labels=['healthy', 'symptomatic', 'COVID-19']).plot()\n",
    "ConfusionMatrixDisplay(cfm, display_labels=['healthy', 'COVID-19']).plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99a40673",
   "metadata": {},
   "outputs": [],
   "source": [
    "cfm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "16a9e3d4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dtype('int64')"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0264777",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.7 ('.venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  },
  "vscode": {
   "interpreter": {
    "hash": "a8d9f7fe00d38372c02b25343d92637c9efc1723885c511df4bb7b75b405b5dc"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
