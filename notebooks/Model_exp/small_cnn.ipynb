{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "72bf15ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import audiomentations\n",
    "from torch_audiomentations import Compose, PitchShift, TimeInversion, AddBackgroundNoise, AddColoredNoise, PolarityInversion\n",
    "import torchaudio\n",
    "import numpy as np\n",
    "import sys\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorboard\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "WRITER_PATH =\"../logs/CNN_S\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b45bd173",
   "metadata": {},
   "source": [
    "# Building the Cough Dataset\n",
    "\n",
    "This is the preprocessing pipeline for all audio samples in the audio dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a6fe837a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adapted from https://github.com/musikalkemist/pytorchforaudio\n",
    "\n",
    "class CoughDataset(Dataset):\n",
    "\n",
    "    def __init__(self,\n",
    "                 annotations_df,\n",
    "                 audio_dir,\n",
    "                 target_sample_rate,\n",
    "                 num_samples,\n",
    "                 device,\n",
    "                ):\n",
    "        self.annotations = annotations_df\n",
    "        self.audio_dir = audio_dir\n",
    "        self.device = device\n",
    "        self.target_sample_rate = target_sample_rate\n",
    "        self.num_samples = num_samples\n",
    "        self.label_dict = {'healthy':0, 'symptomatic':1, 'COVID-19':1}\n",
    "        self.label_weights = self._calculate_weights(annotations_df)\n",
    "\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.annotations)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        audio_sample_path = self._get_audio_sample_path(index)\n",
    "        label = self.label_dict[self._get_audio_sample_label(index)]\n",
    "        signal, sr = torchaudio.load(audio_sample_path)\n",
    "        \n",
    "        signal = signal.to(self.device)\n",
    "        signal = self._resample_if_necessary(signal, sr)\n",
    "        signal = self._mix_down_if_necessary(signal)\n",
    "        signal = self._cut_if_necessary(signal)\n",
    "        signal = self._right_pad_if_necessary(signal)\n",
    "\n",
    "        if \"symptomatic\" == self._get_audio_sample_label(index):\n",
    "            raise ValueError\n",
    "\n",
    "        return signal, label\n",
    "\n",
    "    def _cut_if_necessary(self, signal):\n",
    "        if signal.shape[1] > self.num_samples:\n",
    "            signal = signal[:, :self.num_samples]\n",
    "        return signal\n",
    "\n",
    "    def _right_pad_if_necessary(self, signal):\n",
    "        length_signal = signal.shape[1]\n",
    "        if length_signal < self.num_samples:\n",
    "            num_missing_samples = self.num_samples - length_signal\n",
    "            last_dim_padding = (0, num_missing_samples)\n",
    "            signal = torch.nn.functional.pad(signal, last_dim_padding)\n",
    "        return signal\n",
    "\n",
    "    def _resample_if_necessary(self, signal, sr):\n",
    "        if sr != self.target_sample_rate:\n",
    "            resampler = torchaudio.transforms.Resample(sr, self.target_sample_rate)\n",
    "            signal = resampler(signal)\n",
    "        return signal\n",
    "\n",
    "    def _mix_down_if_necessary(self, signal):\n",
    "        if signal.shape[0] > 1:\n",
    "            signal = torch.mean(signal, dim=0, keepdim=True)\n",
    "        return signal\n",
    "\n",
    "    def _get_audio_sample_path(self, index):\n",
    "        path = os.path.join(self.audio_dir, self.annotations.iloc[index, 0])+\".wav\"\n",
    "        return path\n",
    "\n",
    "\n",
    "    def _get_audio_sample_label(self, index):\n",
    "        return self.annotations.iloc[index, 9]\n",
    "\n",
    "    def _calculate_weights(self, annotation_df):\n",
    "        counts = annotation_df[\"status\"].value_counts()\n",
    "        total = len(annotation_df)\n",
    "        weights = (1-(counts/total))\n",
    "        weights /= weights.sum()\n",
    "        return torch.FloatTensor(weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a92a0fc7",
   "metadata": {},
   "source": [
    "# Oversampled Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ee8831c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "AUDIO_DIR = \"../valid_data/\"\n",
    "SAMPLE_RATE = 16000\n",
    "NUM_SAMPLES = SAMPLE_RATE*10\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    device = \"cuda\"\n",
    "else:\n",
    "    device = \"cpu\"\n",
    "\n",
    "train_df = pd.read_parquet(os.path.join(AUDIO_DIR, \"train_balanced_3500.parquet.gzip\"))\n",
    "val_df = pd.read_parquet(os.path.join(AUDIO_DIR, \"val_balanced_3500.parquet.gzip\"))\n",
    "test_df = pd.read_parquet(os.path.join(AUDIO_DIR, \"test_balanced_3500.parquet.gzip\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2d06d5a",
   "metadata": {},
   "source": [
    "# Normal (3 Class) Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "10e87941",
   "metadata": {},
   "outputs": [],
   "source": [
    "AUDIO_DIR = \"../valid_data/\"\n",
    "SAMPLE_RATE = 16000\n",
    "NUM_SAMPLES = SAMPLE_RATE*10\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    device = \"cuda\"\n",
    "else:\n",
    "    device = \"cpu\"\n",
    "\n",
    "train_df = pd.read_parquet(os.path.join(AUDIO_DIR, \"train_edited.parquet.gzip\"))\n",
    "val_df = pd.read_parquet(os.path.join(AUDIO_DIR, \"val_edited.parquet.gzip\"))\n",
    "test_df = pd.read_parquet(os.path.join(AUDIO_DIR, \"test_edited.parquet.gzip\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1fcdb18",
   "metadata": {},
   "source": [
    "# Small CNN Model (4,000 parameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c258e123",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN(torch.nn.Module):\n",
    "\n",
    "    def __init__(self, drop_p=0.2):\n",
    "        super().__init__()\n",
    "        # 4 conv blocks / flatten / linear / softmax\n",
    "        self.conv1 = nn.Sequential(\n",
    "            nn.Conv2d(\n",
    "                in_channels=1,\n",
    "                out_channels=8,\n",
    "                kernel_size=5,\n",
    "                stride=2,\n",
    "                padding=0\n",
    "            ),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2),\n",
    "            nn.BatchNorm2d(8),\n",
    "            nn.Dropout(p=drop_p),\n",
    "        )\n",
    "\n",
    "        self.conv2 = nn.Sequential(\n",
    "            nn.Conv2d(\n",
    "                in_channels=8,\n",
    "                out_channels=12,\n",
    "                kernel_size=3,\n",
    "                stride=1,\n",
    "                padding=0\n",
    "            ),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2),\n",
    "            nn.BatchNorm2d(12),\n",
    "            nn.Dropout(p=drop_p),\n",
    "        )\n",
    "        self.conv3 = nn.Sequential(\n",
    "            nn.Conv2d(\n",
    "                in_channels=12,\n",
    "                out_channels=12,\n",
    "                kernel_size=3,\n",
    "                stride=1,\n",
    "                padding=0,\n",
    "            ),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2),\n",
    "            nn.BatchNorm2d(12),\n",
    "            nn.Dropout(p=drop_p),\n",
    "        )\n",
    "        self.conv4 = nn.Sequential(\n",
    "            nn.Conv2d(\n",
    "                in_channels=12,\n",
    "                out_channels=16,\n",
    "                kernel_size=3,\n",
    "                stride=1,\n",
    "                padding=0,\n",
    "            ),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2),\n",
    "            nn.BatchNorm2d(16),\n",
    "            nn.Dropout(p=drop_p),\n",
    "        )\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.linear = nn.Linear(224, 2)\n",
    "        self.softmax = nn.Softmax(dim=-1)\n",
    "\n",
    "    def forward(self, input_data):\n",
    "        #nomralization\n",
    "        std = input_data.std()\n",
    "        input_data -= input_data.mean()\n",
    "        input_data /= std\n",
    "        \n",
    "        x = self.conv1(input_data)\n",
    "        x = self.conv2(x)\n",
    "        x = self.conv3(x)\n",
    "        x = self.conv4(x)\n",
    "        x = self.flatten(x)\n",
    "        logits = self.linear(x)\n",
    "        predictions = self.softmax(logits)\n",
    "        return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2c406986",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report, confusion_matrix, f1_score\n",
    "\n",
    "\n",
    "mel_spectrogram = torchaudio.transforms.MelSpectrogram(\n",
    "    sample_rate=SAMPLE_RATE,\n",
    "    n_fft=1024,\n",
    "    hop_length=512,\n",
    "    n_mels=128\n",
    ").to(device)\n",
    "\n",
    "augmentations = Compose(\n",
    "        transforms=[\n",
    "            PitchShift(\n",
    "                mode = \"per_example\",\n",
    "                p=0.5,\n",
    "                sample_rate=SAMPLE_RATE,\n",
    "                output_type=\"tensor\"\n",
    "                ),\n",
    "            TimeInversion(\n",
    "                mode = \"per_example\",\n",
    "                p=0.5,\n",
    "                output_type=\"tensor\"\n",
    "            ),\n",
    "            AddColoredNoise(\n",
    "                mode = \"per_example\",\n",
    "                p=0.5,\n",
    "                sample_rate=SAMPLE_RATE,\n",
    "                output_type=\"tensor\"\n",
    "            ),\n",
    "            PolarityInversion(\n",
    "                mode=\"per_example\",\n",
    "                p=0.5,\n",
    "                output_type=\"tensor\"\n",
    "            ), \n",
    "\n",
    "        ], output_type=\"tensor\"\n",
    "    )\n",
    "\n",
    "def create_data_loader(train_data, batch_size):\n",
    "    train_dataloader = DataLoader(train_data, batch_size=batch_size)\n",
    "    print('train_dataloader finished: ', train_dataloader)\n",
    "    return train_dataloader\n",
    "\n",
    "def count_correct(logits, y_true):\n",
    "    y_pred = torch.argmax(logits, axis = 1)\n",
    "    return torch.sum(y_pred==y_true)\n",
    "\n",
    "def train_single_epoch(model, train_data_loader, val_data_loader, loss_fn, optimiser, device, do_augment=False):\n",
    "    total_loss_train = 0.0\n",
    "    correct_pred_train = 0.0\n",
    "    total_pred_train = 0\n",
    "\n",
    "    train_trues = []\n",
    "    train_preds = []\n",
    "\n",
    "    for x_batch, y_batch in tqdm(train_data_loader):\n",
    "        x_batch, y_batch = x_batch.to(device), y_batch.to(device)\n",
    "\n",
    "        if do_augment:\n",
    "            x_batch = augmentations(x_batch, SAMPLE_RATE)\n",
    "        \n",
    "\n",
    "        x_batch = x_batch.reshape(-1, x_batch.shape[-1])\n",
    "        x_batch = mel_spectrogram(x_batch)\n",
    "        x_batch = x_batch.reshape(x_batch.shape[0], 1, x_batch.shape[-2], x_batch.shape[-1])\n",
    "        \n",
    "        # calculate loss\n",
    "        y_pred = model(x_batch)\n",
    "        loss = loss_fn(y_pred, y_batch)\n",
    "\n",
    "        # add to list for f1 score\n",
    "        train_trues.append(y_batch.cpu())\n",
    "        train_preds.append(y_pred.cpu())\n",
    "        \n",
    "        correct_pred_train += count_correct(y_pred, y_batch)\n",
    "        total_pred_train += y_batch.shape[0]\n",
    "\n",
    "        # backpropagate error and update weights\n",
    "        optimiser.zero_grad()\n",
    "        loss.backward()\n",
    "        optimiser.step()\n",
    "        total_loss_train += loss.item()\n",
    "\n",
    "        \n",
    "        \n",
    "    print(f\"Training loss: {total_loss_train}, Training accuracy : {correct_pred_train/total_pred_train}\")\n",
    "    #print normalized loss\n",
    "    print(f\"Training loss normalized: {total_loss_train/len(train_data_loader)}\")\n",
    "    # print f1 score, precision, recall\n",
    "    # print(classification_report(torch.cat(train_trues).detach().cpu().numpy(), torch.argmax(torch.cat(train_preds), axis=1).detach().cpu().numpy()))\n",
    "    \n",
    "    total_loss_val = 0.0\n",
    "    correct_pred_val = 0.0\n",
    "    total_pred_val = 0\n",
    "\n",
    "    val_trues = []\n",
    "    val_preds = []\n",
    "\n",
    "    for x_batch, y_batch in tqdm(val_data_loader):\n",
    "        with torch.no_grad():\n",
    "            x_batch, y_batch = x_batch.to(device), y_batch.to(device)\n",
    "\n",
    "            x_batch = x_batch.reshape(-1, x_batch.shape[-1])\n",
    "            x_batch = mel_spectrogram(x_batch)\n",
    "            x_batch = x_batch.reshape(x_batch.shape[0], 1, x_batch.shape[-2], x_batch.shape[-1])\n",
    "\n",
    "            y_pred = model(x_batch)\n",
    "            loss = loss_fn(y_pred, y_batch)\n",
    "            total_loss_val += loss.item() \n",
    "            \n",
    "            val_trues.append(y_batch.cpu())\n",
    "            val_preds.append(y_pred.cpu())\n",
    "\n",
    "        correct_pred_val += count_correct(y_pred, y_batch)\n",
    "        total_pred_val += y_batch.shape[0]\n",
    "        \n",
    "    print(f\"Validataion loss: {total_loss_val}, Validation accuracy : {correct_pred_val/total_pred_val}\")\n",
    "    #print normalized loss\n",
    "    print(f\"Validation loss normalized: {total_loss_val/len(val_data_loader)}\")\n",
    "    #print f1 score, confusion matrix and precision, recall using sklearn and true and predicted values\n",
    "    # print(classification_report(torch.cat(val_trues).detach().cpu().numpy(), torch.argmax(torch.cat(val_preds), axis=1).detach().cpu().numpy()))\n",
    "    return total_loss_train/len(train_data_loader), correct_pred_train/total_pred_train, total_loss_val/len(val_data_loader), correct_pred_val/total_pred_val\n",
    "\n",
    "\n",
    "    \n",
    "def train(model, train_data_loader, val_data_loader, loss_fn, optimiser, device, epochs, do_augment):\n",
    "    writer = SummaryWriter(WRITER_PATH)\n",
    "    for i in range(epochs):\n",
    "        print(f\"Epoch {i+1}\")\n",
    "        train_loss, train_acc, val_loss, val_acc = train_single_epoch(model, train_data_loader, val_data_loader, loss_fn, optimiser, device)\n",
    "        writer.add_scalar(\"train/accuracy\", train_acc, i)\n",
    "        writer.add_scalar(\"train/loss\", train_loss, i)\n",
    "        writer.add_scalar(\"validation/accuracy\", val_acc, i)\n",
    "        writer.add_scalar(\"validation/loss\", val_loss, i)\n",
    "        \n",
    "        path = os.path.join(MODEL_FOLDER, f\"epoch_{i}.pth\")\n",
    "        torch.save(model.state_dict(), path)\n",
    "        print(f\"Saved at {path}\")\n",
    "        print(\"---------------------------\")\n",
    "    print(\"Finished training\")\n",
    "    print(\"---------------------------\")\n",
    "\n",
    "    \n",
    "    \n",
    "def evaluate(model, eval_data_loader, loss_fn, device):\n",
    "    print(\"Evaluating model\")\n",
    "    total_loss = 0.0\n",
    "    correct_pred = 0.0\n",
    "    total_pred = 0\n",
    "    for x_batch, y_batch in tqdm(eval_data_loader):\n",
    "        with torch.no_grad():\n",
    "            x_batch, y_batch = x_batch.to(device), y_batch.to(device)\n",
    "\n",
    "            x_batch = x_batch.reshape(-1, x_batch.shape[-1])\n",
    "            x_batch = mel_spectrogram(x_batch)\n",
    "            x_batch = x_batch.reshape(x_batch.shape[0], 1, x_batch.shape[-2], x_batch.shape[-1])\n",
    "\n",
    "            y_pred = model(x_batch)\n",
    "            loss = loss_fn(y_pred, y_batch)\n",
    "\n",
    "            correct_pred += count_correct(y_pred, y_batch)\n",
    "            total_pred += y_batch.shape[0]\n",
    "\n",
    "            total_loss += loss.item()\n",
    "\n",
    "    print(f\"Evaluation loss: {total_loss}, Evaluation accuracy : {correct_pred/total_pred}\")\n",
    "    # print normalized loss\n",
    "    print(f\"Evaluation loss normalized: {total_loss/len(eval_data_loader)}\")\n",
    "    print(\"---------------------------\")        \n",
    "            \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b393f0aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train data:  <__main__.CoughDataset object at 0x285da1780>\n",
      "train_dataloader finished:  <torch.utils.data.dataloader.DataLoader object at 0x285da1a80>\n",
      "train_dataloader finished:  <torch.utils.data.dataloader.DataLoader object at 0x285da1180>\n",
      "train_dataloader finished:  <torch.utils.data.dataloader.DataLoader object at 0x285e4f970>\n"
     ]
    }
   ],
   "source": [
    "BATCH_SIZE = 256\n",
    "EPOCHS = 50\n",
    "MODEL_FOLDER = '../models/CNN_S'\n",
    "\n",
    "train_data = CoughDataset(train_df,\n",
    "                        AUDIO_DIR,\n",
    "                        SAMPLE_RATE,\n",
    "                        NUM_SAMPLES,\n",
    "                        device,\n",
    "                        )\n",
    "print('train data: ', train_data)\n",
    "\n",
    "val_data = CoughDataset(val_df,\n",
    "                        AUDIO_DIR,\n",
    "                        SAMPLE_RATE,\n",
    "                        NUM_SAMPLES,\n",
    "                        device)\n",
    "\n",
    "test_data = CoughDataset(test_df,\n",
    "                        AUDIO_DIR,\n",
    "                        SAMPLE_RATE,\n",
    "                        NUM_SAMPLES,\n",
    "                        device)\n",
    "\n",
    "train_dataloader = create_data_loader(train_data, BATCH_SIZE)\n",
    "val_dataloader = create_data_loader(val_data, BATCH_SIZE)\n",
    "test_dataloader = create_data_loader(val_data, BATCH_SIZE)\n",
    "\n",
    "# construct model and assign it to device\n",
    "model = CNN().to(device)\n",
    "\n",
    "# initialise loss funtion + optimiser\n",
    "#loss_fn = nn.CrossEntropyLoss(weight=train_data.label_weights)\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimiser = torch.optim.Adam(model.parameters(), lr=1e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "849e78e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 25/25 [00:58<00:00,  2.36s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss: 17.669932782649994, Training accuracy : 0.4767349660396576\n",
      "Training loss normalized: 0.7067973113059998\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4/4 [00:07<00:00,  1.77s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validataion loss: 3.028381884098053, Validation accuracy : 0.25401929020881653\n",
      "Validation loss normalized: 0.7570954710245132\n",
      "Saved at ../models/CNN_S/epoch_0.pth\n",
      "---------------------------\n",
      "Epoch 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  8%|▊         | 2/25 [00:04<00:51,  2.25s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m train(model, train_dataloader, val_dataloader, loss_fn, optimiser, device, EPOCHS, do_augment\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n",
      "Cell \u001b[0;32mIn[6], line 130\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(model, train_data_loader, val_data_loader, loss_fn, optimiser, device, epochs, do_augment)\u001b[0m\n\u001b[1;32m    128\u001b[0m \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(epochs):\n\u001b[1;32m    129\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mEpoch \u001b[39m\u001b[39m{\u001b[39;00mi\u001b[39m+\u001b[39m\u001b[39m1\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n\u001b[0;32m--> 130\u001b[0m     train_loss, train_acc, val_loss, val_acc \u001b[39m=\u001b[39m train_single_epoch(model, train_data_loader, val_data_loader, loss_fn, optimiser, device)\n\u001b[1;32m    131\u001b[0m     writer\u001b[39m.\u001b[39madd_scalar(\u001b[39m\"\u001b[39m\u001b[39mtrain/accuracy\u001b[39m\u001b[39m\"\u001b[39m, train_acc, i)\n\u001b[1;32m    132\u001b[0m     writer\u001b[39m.\u001b[39madd_scalar(\u001b[39m\"\u001b[39m\u001b[39mtrain/loss\u001b[39m\u001b[39m\"\u001b[39m, train_loss, i)\n",
      "Cell \u001b[0;32mIn[6], line 56\u001b[0m, in \u001b[0;36mtrain_single_epoch\u001b[0;34m(model, train_data_loader, val_data_loader, loss_fn, optimiser, device, do_augment)\u001b[0m\n\u001b[1;32m     53\u001b[0m train_trues \u001b[39m=\u001b[39m []\n\u001b[1;32m     54\u001b[0m train_preds \u001b[39m=\u001b[39m []\n\u001b[0;32m---> 56\u001b[0m \u001b[39mfor\u001b[39;00m x_batch, y_batch \u001b[39min\u001b[39;00m tqdm(train_data_loader):\n\u001b[1;32m     57\u001b[0m     x_batch, y_batch \u001b[39m=\u001b[39m x_batch\u001b[39m.\u001b[39mto(device), y_batch\u001b[39m.\u001b[39mto(device)\n\u001b[1;32m     59\u001b[0m     \u001b[39mif\u001b[39;00m do_augment:\n",
      "File \u001b[0;32m~/Documents/cs230/coughvid/.venv/lib/python3.10/site-packages/tqdm/std.py:1195\u001b[0m, in \u001b[0;36mtqdm.__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1192\u001b[0m time \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_time\n\u001b[1;32m   1194\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m-> 1195\u001b[0m     \u001b[39mfor\u001b[39;00m obj \u001b[39min\u001b[39;00m iterable:\n\u001b[1;32m   1196\u001b[0m         \u001b[39myield\u001b[39;00m obj\n\u001b[1;32m   1197\u001b[0m         \u001b[39m# Update and possibly print the progressbar.\u001b[39;00m\n\u001b[1;32m   1198\u001b[0m         \u001b[39m# Note: does not call self.update(1) for speed optimisation.\u001b[39;00m\n",
      "File \u001b[0;32m~/Documents/cs230/coughvid/.venv/lib/python3.10/site-packages/torch/utils/data/dataloader.py:628\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    625\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_sampler_iter \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    626\u001b[0m     \u001b[39m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    627\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_reset()  \u001b[39m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 628\u001b[0m data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_next_data()\n\u001b[1;32m    629\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[1;32m    630\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_dataset_kind \u001b[39m==\u001b[39m _DatasetKind\u001b[39m.\u001b[39mIterable \u001b[39mand\u001b[39;00m \\\n\u001b[1;32m    631\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m \\\n\u001b[1;32m    632\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m>\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[0;32m~/Documents/cs230/coughvid/.venv/lib/python3.10/site-packages/torch/utils/data/dataloader.py:671\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    669\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_next_data\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[1;32m    670\u001b[0m     index \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_next_index()  \u001b[39m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m--> 671\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_dataset_fetcher\u001b[39m.\u001b[39;49mfetch(index)  \u001b[39m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m    672\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_pin_memory:\n\u001b[1;32m    673\u001b[0m         data \u001b[39m=\u001b[39m _utils\u001b[39m.\u001b[39mpin_memory\u001b[39m.\u001b[39mpin_memory(data, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[0;32m~/Documents/cs230/coughvid/.venv/lib/python3.10/site-packages/torch/utils/data/_utils/fetch.py:58\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     56\u001b[0m         data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset\u001b[39m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[1;32m     57\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m---> 58\u001b[0m         data \u001b[39m=\u001b[39m [\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset[idx] \u001b[39mfor\u001b[39;00m idx \u001b[39min\u001b[39;00m possibly_batched_index]\n\u001b[1;32m     59\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m     60\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[0;32m~/Documents/cs230/coughvid/.venv/lib/python3.10/site-packages/torch/utils/data/_utils/fetch.py:58\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     56\u001b[0m         data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset\u001b[39m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[1;32m     57\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m---> 58\u001b[0m         data \u001b[39m=\u001b[39m [\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdataset[idx] \u001b[39mfor\u001b[39;00m idx \u001b[39min\u001b[39;00m possibly_batched_index]\n\u001b[1;32m     59\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m     60\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset[possibly_batched_index]\n",
      "Cell \u001b[0;32mIn[3], line 27\u001b[0m, in \u001b[0;36mCoughDataset.__getitem__\u001b[0;34m(self, index)\u001b[0m\n\u001b[1;32m     25\u001b[0m audio_sample_path \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_get_audio_sample_path(index)\n\u001b[1;32m     26\u001b[0m label \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlabel_dict[\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_get_audio_sample_label(index)]\n\u001b[0;32m---> 27\u001b[0m signal, sr \u001b[39m=\u001b[39m torchaudio\u001b[39m.\u001b[39;49mload(audio_sample_path)\n\u001b[1;32m     29\u001b[0m signal \u001b[39m=\u001b[39m signal\u001b[39m.\u001b[39mto(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdevice)\n\u001b[1;32m     30\u001b[0m signal \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_resample_if_necessary(signal, sr)\n",
      "File \u001b[0;32m~/Documents/cs230/coughvid/.venv/lib/python3.10/site-packages/torchaudio/backend/soundfile_backend.py:205\u001b[0m, in \u001b[0;36mload\u001b[0;34m(filepath, frame_offset, num_frames, normalize, channels_first, format)\u001b[0m\n\u001b[1;32m    123\u001b[0m \u001b[39m@_mod_utils\u001b[39m\u001b[39m.\u001b[39mrequires_soundfile()\n\u001b[1;32m    124\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mload\u001b[39m(\n\u001b[1;32m    125\u001b[0m     filepath: \u001b[39mstr\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    130\u001b[0m     \u001b[39mformat\u001b[39m: Optional[\u001b[39mstr\u001b[39m] \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m,\n\u001b[1;32m    131\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tuple[torch\u001b[39m.\u001b[39mTensor, \u001b[39mint\u001b[39m]:\n\u001b[1;32m    132\u001b[0m     \u001b[39m\"\"\"Load audio data from file.\u001b[39;00m\n\u001b[1;32m    133\u001b[0m \n\u001b[1;32m    134\u001b[0m \u001b[39m    Note:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    203\u001b[0m \u001b[39m            `[channel, time]` else `[time, channel]`.\u001b[39;00m\n\u001b[1;32m    204\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 205\u001b[0m     \u001b[39mwith\u001b[39;00m soundfile\u001b[39m.\u001b[39;49mSoundFile(filepath, \u001b[39m\"\u001b[39;49m\u001b[39mr\u001b[39;49m\u001b[39m\"\u001b[39;49m) \u001b[39mas\u001b[39;00m file_:\n\u001b[1;32m    206\u001b[0m         \u001b[39mif\u001b[39;00m file_\u001b[39m.\u001b[39mformat \u001b[39m!=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mWAV\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mor\u001b[39;00m normalize:\n\u001b[1;32m    207\u001b[0m             dtype \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mfloat32\u001b[39m\u001b[39m\"\u001b[39m\n",
      "File \u001b[0;32m~/Documents/cs230/coughvid/.venv/lib/python3.10/site-packages/soundfile.py:655\u001b[0m, in \u001b[0;36mSoundFile.__init__\u001b[0;34m(self, file, mode, samplerate, channels, subtype, endian, format, closefd)\u001b[0m\n\u001b[1;32m    652\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_mode \u001b[39m=\u001b[39m mode\n\u001b[1;32m    653\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_info \u001b[39m=\u001b[39m _create_info_struct(file, mode, samplerate, channels,\n\u001b[1;32m    654\u001b[0m                                  \u001b[39mformat\u001b[39m, subtype, endian)\n\u001b[0;32m--> 655\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_file \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_open(file, mode_int, closefd)\n\u001b[1;32m    656\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mset\u001b[39m(mode)\u001b[39m.\u001b[39missuperset(\u001b[39m'\u001b[39m\u001b[39mr+\u001b[39m\u001b[39m'\u001b[39m) \u001b[39mand\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mseekable():\n\u001b[1;32m    657\u001b[0m     \u001b[39m# Move write position to 0 (like in Python file objects)\u001b[39;00m\n\u001b[1;32m    658\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mseek(\u001b[39m0\u001b[39m)\n",
      "File \u001b[0;32m~/Documents/cs230/coughvid/.venv/lib/python3.10/site-packages/soundfile.py:1202\u001b[0m, in \u001b[0;36mSoundFile._open\u001b[0;34m(self, file, mode_int, closefd)\u001b[0m\n\u001b[1;32m   1200\u001b[0m         \u001b[39melse\u001b[39;00m:\n\u001b[1;32m   1201\u001b[0m             file \u001b[39m=\u001b[39m file\u001b[39m.\u001b[39mencode(_sys\u001b[39m.\u001b[39mgetfilesystemencoding())\n\u001b[0;32m-> 1202\u001b[0m     file_ptr \u001b[39m=\u001b[39m openfunction(file, mode_int, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_info)\n\u001b[1;32m   1203\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39misinstance\u001b[39m(file, \u001b[39mint\u001b[39m):\n\u001b[1;32m   1204\u001b[0m     file_ptr \u001b[39m=\u001b[39m _snd\u001b[39m.\u001b[39msf_open_fd(file, mode_int, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_info, closefd)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "train(model, train_dataloader, val_dataloader, loss_fn, optimiser, device, EPOCHS, do_augment=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "id": "0be4c890",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 72/72 [00:10<00:00,  6.73it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation loss: 57.29608428478241, Evaluation accuracy : 0.753484308719635\n",
      "Evaluation loss normalized: 0.7957789483997557\n",
      "---------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "#set model to be the epoch 10 model in the models folder\n",
    "#model.load_state_dict(torch.load('../models/CNN_S/epoch_9.pth'))\n",
    "\n",
    "# evaluate model with test data using the loaded model\n",
    "trues, preds = evaluate(model, test_dataloader, loss_fn, device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "id": "c4270da5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import ConfusionMatrixDisplay, confusion_matrix\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "id": "92ad25a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import plot_confusion_matrix\n",
    "\n",
    "mel_spectrogram = torchaudio.transforms.MelSpectrogram(\n",
    "    sample_rate=SAMPLE_RATE,\n",
    "    n_fft=1024,\n",
    "    hop_length=512,\n",
    "    n_mels=128\n",
    ")\n",
    "\n",
    "def evaluate_confusion(model, eval_data_loader, device):\n",
    "    trues = []\n",
    "    preds =[]\n",
    "    for x_batch, y_batch in tqdm(eval_data_loader):\n",
    "        with torch.no_grad():\n",
    "            x_batch, y_batch = x_batch.to(device), y_batch.to(device)\n",
    "\n",
    "            x_batch = x_batch.reshape(-1, x_batch.shape[-1])\n",
    "            x_batch = mel_spectrogram(x_batch)\n",
    "            x_batch = x_batch.reshape(x_batch.shape[0], 1, x_batch.shape[-2], x_batch.shape[-1])\n",
    "            \n",
    "            y_pred = model(x_batch)\n",
    "\n",
    "            trues += torch.clamp(y_batch, max=1)\n",
    "            preds += torch.clamp(torch.argmax(y_pred, axis = 1), max=1)            \n",
    "            \n",
    "    return np.array(trues), np.array(preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "id": "494fdd01",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 72/72 [00:11<00:00,  6.29it/s]\n"
     ]
    }
   ],
   "source": [
    "trues, preds = evaluate_confusion(model, test_dataloader, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2eba3ddf",
   "metadata": {},
   "outputs": [],
   "source": [
    "cfm = confusion_matrix(trues, preds)\n",
    "ConfusionMatrixDisplay(cfm, display_labels=['healthy', 'COVID-19']).plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99a40673",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print classification report\n",
    "from sklearn.metrics import classification_report\n",
    "print(classification_report(trues, preds))\n",
    "\n",
    "# calculate f1 score\n",
    "from sklearn.metrics import f1_score\n",
    "print('f1: ', f1_score(trues, preds, average='macro'))\n",
    "\n",
    "#calculate and graph ROC AUC\n",
    "from sklearn.metrics import roc_auc_score, plot_roc_curve, roc_curve\n",
    "r_auc = roc_auc_score(trues, preds)\n",
    "print('roc: ', r_auc)\n",
    "\n",
    "fpr, tpr, thresholds = roc_curve(trues, preds)\n",
    "plt.plot(fpr, tpr, label='CNN prediction (AUROC = %0.3f)' % r_auc)\n",
    "plt.title('ROC Plot')\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.6 ('.venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "vscode": {
   "interpreter": {
    "hash": "9f480ccb6be28d4b7a840fe3b1174df64fdfbdad14d75b02fbd7e3b419aecfda"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
